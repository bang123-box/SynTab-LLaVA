[2024-11-03 10:00:04,663] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:06,264] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-11-03 10:00:06,264] [INFO] [runner.py:571:main] cmd = /opt/conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=21055 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path /home/zbb/modelscope/hub/models--lmsys--vicuna-7b-v1.5/snapshots/3321f76e3f527bd14065daf69dad9344000a201d --version plain --data_path pretrain.jsonl --image_folder none --highres_vision_tower /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 --lowres_vision_tower /home/zbb/modelscope/hub/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./pretrained_mm_projector/Pre_MP_7b_Conv_Clip --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy steps --save_steps 6000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2560 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
[2024-11-03 10:00:08,325] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:09,829] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1
[2024-11-03 10:00:09,829] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1
[2024-11-03 10:00:09,829] [INFO] [launch.py:138:main] 0 NCCL_P2P_LEVEL=NVL
[2024-11-03 10:00:09,829] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-11-03 10:00:09,829] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-11-03 10:00:09,829] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1
[2024-11-03 10:00:09,829] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1
[2024-11-03 10:00:09,829] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.17.1-1
[2024-11-03 10:00:09,829] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-11-03 10:00:09,829] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-11-03 10:00:09,829] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-11-03 10:00:09,829] [INFO] [launch.py:163:main] dist_world_size=8
[2024-11-03 10:00:09,829] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-11-03 10:00:12,994] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:13,020] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:13,026] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:13,040] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:13,044] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:13,053] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:13,085] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:13,090] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-03 10:00:14,253] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 10:00:14,271] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 10:00:14,301] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 10:00:14,348] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 10:00:14,348] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-03 10:00:14,349] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 10:00:14,352] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 10:00:14,366] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-03 10:00:14,382] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.05s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.71s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.24s/it]
entering load model, load /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.34s/it]
entering load model, load /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536
entering load model, load /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.26s/it]
entering load model, load /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]
entering load model, load /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.07s/it]
entering load model, load /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536
entering load model, load /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536
entering load model, load /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536
Some weights of ConvNextModel were not initialized from the model checkpoint at /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 and are newly initialized: ['layernorm.bias', 'layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
entering load model, load /home/zbb/modelscope/hub/clip-vit-large-patch14-336
Some weights of ConvNextModel were not initialized from the model checkpoint at /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 and are newly initialized: ['layernorm.bias', 'layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of ConvNextModel were not initialized from the model checkpoint at /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 and are newly initialized: ['layernorm.bias', 'layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
entering load model, load /home/zbb/modelscope/hub/clip-vit-large-patch14-336
entering load model, load /home/zbb/modelscope/hub/clip-vit-large-patch14-336
Some weights of ConvNextModel were not initialized from the model checkpoint at /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 and are newly initialized: ['layernorm.bias', 'layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
entering load model, load /home/zbb/modelscope/hub/clip-vit-large-patch14-336
Some weights of ConvNextModel were not initialized from the model checkpoint at /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 and are newly initialized: ['layernorm.bias', 'layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of ConvNextModel were not initialized from the model checkpoint at /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 and are newly initialized: ['layernorm.bias', 'layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
entering load model, load /home/zbb/modelscope/hub/clip-vit-large-patch14-336
Some weights of ConvNextModel were not initialized from the model checkpoint at /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 and are newly initialized: ['layernorm.bias', 'layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
entering load model, load /home/zbb/modelscope/hub/clip-vit-large-patch14-336
entering load model, load /home/zbb/modelscope/hub/clip-vit-large-patch14-336
Some weights of ConvNextModel were not initialized from the model checkpoint at /home/zbb/modelscope/hub/ConvLLaVA-ConvNeXt-1536 and are newly initialized: ['layernorm.bias', 'layernorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
entering load model, load /home/zbb/modelscope/hub/clip-vit-large-patch14-336
Formatting inputs...Skip in lazy mode
  0%|          | 0/588 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3628 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (8558 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3529 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5512 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5679 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5677 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5498 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4199 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5150 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5462 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3173 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (6779 > 2560). Running this sequence through the model will result in indexing errors
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Token indices sequence length is longer than the specified maximum sequence length for this model (3083 > 2560). Running this sequence through the model will result in indexing errors
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Token indices sequence length is longer than the specified maximum sequence length for this model (5673 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3171 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3246 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4751 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5678 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2953 > 2560). Running this sequence through the model will result in indexing errors
/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/opt/conda/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/588 [00:34<5:42:00, 34.96s/it]                                                 {'loss': 4.1025, 'learning_rate': 5.555555555555555e-05, 'epoch': 0.0}
  0%|          | 1/588 [00:34<5:42:00, 34.96s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2795 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (3621 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5020 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4203 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2683 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (19117 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (4613 > 2560). Running this sequence through the model will result in indexing errors
  0%|          | 2/588 [01:05<5:14:48, 32.23s/it]                                                 {'loss': 4.0197, 'learning_rate': 0.0001111111111111111, 'epoch': 0.0}
  0%|          | 2/588 [01:05<5:14:48, 32.23s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3631 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (5509 > 2560). Running this sequence through the model will result in indexing errors
  1%|          | 3/588 [01:35<5:05:37, 31.35s/it]                                                 {'loss': 2.2265, 'learning_rate': 0.00016666666666666666, 'epoch': 0.01}
  1%|          | 3/588 [01:35<5:05:37, 31.35s/it]  1%|          | 4/588 [02:05<5:01:01, 30.93s/it]                                                 {'loss': 1.5537, 'learning_rate': 0.0002222222222222222, 'epoch': 0.01}
  1%|          | 4/588 [02:05<5:01:01, 30.93s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (5661 > 2560). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (11330 > 2560). Running this sequence through the model will result in indexing errors
  1%|          | 5/588 [02:36<4:58:25, 30.71s/it]                                                 {'loss': 1.3873, 'learning_rate': 0.0002777777777777778, 'epoch': 0.01}
  1%|          | 5/588 [02:36<4:58:25, 30.71s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (6254 > 2560). Running this sequence through the model will result in indexing errors
  1%|          | 6/588 [03:06<4:57:15, 30.65s/it]                                                 {'loss': 1.3469, 'learning_rate': 0.0003333333333333333, 'epoch': 0.01}
  1%|          | 6/588 [03:06<4:57:15, 30.65s/it]  1%|          | 7/588 [03:36<4:55:37, 30.53s/it]                                                 {'loss': 1.2621, 'learning_rate': 0.0003888888888888889, 'epoch': 0.01}
  1%|          | 7/588 [03:36<4:55:37, 30.53s/it]  1%|▏         | 8/588 [04:07<4:53:53, 30.40s/it]                                                 {'loss': 1.2031, 'learning_rate': 0.0004444444444444444, 'epoch': 0.01}
  1%|▏         | 8/588 [04:07<4:53:53, 30.40s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (5651 > 2560). Running this sequence through the model will result in indexing errors
  2%|▏         | 9/588 [04:37<4:53:11, 30.38s/it]                                                 {'loss': 0.9604, 'learning_rate': 0.0005, 'epoch': 0.02}
  2%|▏         | 9/588 [04:37<4:53:11, 30.38s/it]  2%|▏         | 10/588 [05:07<4:51:42, 30.28s/it]                                                  {'loss': 0.9661, 'learning_rate': 0.0005555555555555556, 'epoch': 0.02}
  2%|▏         | 10/588 [05:07<4:51:42, 30.28s/it]  2%|▏         | 11/588 [05:37<4:51:26, 30.31s/it]                                                  {'loss': 0.872, 'learning_rate': 0.0006111111111111112, 'epoch': 0.02}
  2%|▏         | 11/588 [05:37<4:51:26, 30.31s/it]  2%|▏         | 12/588 [06:08<4:51:00, 30.31s/it]                                                  {'loss': 0.8656, 'learning_rate': 0.0006666666666666666, 'epoch': 0.02}
  2%|▏         | 12/588 [06:08<4:51:00, 30.31s/it]  2%|▏         | 13/588 [06:38<4:49:45, 30.24s/it]                                                  {'loss': 0.849, 'learning_rate': 0.0007222222222222222, 'epoch': 0.02}
  2%|▏         | 13/588 [06:38<4:49:45, 30.24s/it]  2%|▏         | 14/588 [07:08<4:49:23, 30.25s/it]                                                  {'loss': 0.8225, 'learning_rate': 0.0007777777777777778, 'epoch': 0.02}
  2%|▏         | 14/588 [07:08<4:49:23, 30.25s/it]  3%|▎         | 15/588 [07:38<4:49:07, 30.28s/it]                                                  {'loss': 0.7872, 'learning_rate': 0.0008333333333333334, 'epoch': 0.03}
  3%|▎         | 15/588 [07:38<4:49:07, 30.28s/it]  3%|▎         | 16/588 [08:09<4:49:03, 30.32s/it]                                                  {'loss': 0.7897, 'learning_rate': 0.0008888888888888888, 'epoch': 0.03}
  3%|▎         | 16/588 [08:09<4:49:03, 30.32s/it]  3%|▎         | 17/588 [08:39<4:48:21, 30.30s/it]                                                  {'loss': 0.8244, 'learning_rate': 0.0009444444444444445, 'epoch': 0.03}
  3%|▎         | 17/588 [08:39<4:48:21, 30.30s/it]  3%|▎         | 18/588 [09:09<4:47:48, 30.30s/it]                                                  {'loss': 0.8792, 'learning_rate': 0.001, 'epoch': 0.03}
  3%|▎         | 18/588 [09:09<4:47:48, 30.30s/it]  3%|▎         | 19/588 [09:40<4:47:25, 30.31s/it]                                                  {'loss': 0.8103, 'learning_rate': 0.000999992405679119, 'epoch': 0.03}
  3%|▎         | 19/588 [09:40<4:47:25, 30.31s/it]  3%|▎         | 20/588 [10:10<4:47:07, 30.33s/it]                                                  {'loss': 0.8298, 'learning_rate': 0.0009999696229471716, 'epoch': 0.03}
  3%|▎         | 20/588 [10:10<4:47:07, 30.33s/it]  4%|▎         | 21/588 [10:40<4:46:46, 30.35s/it]                                                  {'loss': 0.7599, 'learning_rate': 0.0009999316524962345, 'epoch': 0.04}
  4%|▎         | 21/588 [10:40<4:46:46, 30.35s/it]  4%|▎         | 22/588 [11:11<4:46:04, 30.33s/it]                                                  {'loss': 0.7876, 'learning_rate': 0.0009998784954797473, 'epoch': 0.04}
  4%|▎         | 22/588 [11:11<4:46:04, 30.33s/it]  4%|▍         | 23/588 [11:41<4:45:55, 30.36s/it]                                                  {'loss': 0.7436, 'learning_rate': 0.0009998101535124758, 'epoch': 0.04}
  4%|▍         | 23/588 [11:41<4:45:55, 30.36s/it]  4%|▍         | 24/588 [12:12<4:45:56, 30.42s/it]                                                  {'loss': 0.7515, 'learning_rate': 0.000999726628670463, 'epoch': 0.04}
  4%|▍         | 24/588 [12:12<4:45:56, 30.42s/it]  4%|▍         | 25/588 [12:42<4:45:36, 30.44s/it]                                                  {'loss': 0.6723, 'learning_rate': 0.0009996279234909672, 'epoch': 0.04}
  4%|▍         | 25/588 [12:42<4:45:36, 30.44s/it]  4%|▍         | 26/588 [13:13<4:44:50, 30.41s/it]                                                  {'loss': 0.7881, 'learning_rate': 0.000999514040972383, 'epoch': 0.04}
  4%|▍         | 26/588 [13:13<4:44:50, 30.41s/it]  5%|▍         | 27/588 [13:43<4:43:49, 30.36s/it]                                                  {'loss': 0.7793, 'learning_rate': 0.0009993849845741523, 'epoch': 0.05}
  5%|▍         | 27/588 [13:43<4:43:49, 30.36s/it]  5%|▍         | 28/588 [14:13<4:44:09, 30.45s/it]                                                  {'loss': 0.766, 'learning_rate': 0.000999240758216658, 'epoch': 0.05}
  5%|▍         | 28/588 [14:13<4:44:09, 30.45s/it]  5%|▍         | 29/588 [14:44<4:43:38, 30.44s/it]                                                  {'loss': 0.7229, 'learning_rate': 0.000999081366281105, 'epoch': 0.05}
  5%|▍         | 29/588 [14:44<4:43:38, 30.44s/it]  5%|▌         | 30/588 [15:14<4:42:32, 30.38s/it]                                                  {'loss': 0.7511, 'learning_rate': 0.0009989068136093873, 'epoch': 0.05}
  5%|▌         | 30/588 [15:14<4:42:32, 30.38s/it]  5%|▌         | 31/588 [15:44<4:41:57, 30.37s/it]                                                  {'loss': 0.7393, 'learning_rate': 0.0009987171055039407, 'epoch': 0.05}
  5%|▌         | 31/588 [15:44<4:41:57, 30.37s/it]  5%|▌         | 32/588 [16:15<4:41:24, 30.37s/it]                                                  {'loss': 0.7348, 'learning_rate': 0.0009985122477275824, 'epoch': 0.05}
  5%|▌         | 32/588 [16:15<4:41:24, 30.37s/it]  6%|▌         | 33/588 [16:45<4:40:37, 30.34s/it]                                                  {'loss': 0.7327, 'learning_rate': 0.000998292246503335, 'epoch': 0.06}
  6%|▌         | 33/588 [16:45<4:40:37, 30.34s/it]  6%|▌         | 34/588 [17:15<4:40:08, 30.34s/it]                                                  {'loss': 0.7228, 'learning_rate': 0.000998057108514238, 'epoch': 0.06}
  6%|▌         | 34/588 [17:15<4:40:08, 30.34s/it]  6%|▌         | 35/588 [17:46<4:39:52, 30.37s/it]                                                  {'loss': 0.7184, 'learning_rate': 0.0009978068409031448, 'epoch': 0.06}
  6%|▌         | 35/588 [17:46<4:39:52, 30.37s/it]  6%|▌         | 36/588 [18:16<4:39:12, 30.35s/it]                                                  {'loss': 0.7644, 'learning_rate': 0.0009975414512725057, 'epoch': 0.06}
  6%|▌         | 36/588 [18:16<4:39:12, 30.35s/it]  6%|▋         | 37/588 [18:46<4:38:31, 30.33s/it]                                                  {'loss': 0.8049, 'learning_rate': 0.0009972609476841367, 'epoch': 0.06}
  6%|▋         | 37/588 [18:46<4:38:31, 30.33s/it]  6%|▋         | 38/588 [19:17<4:38:14, 30.35s/it]                                                  {'loss': 0.7011, 'learning_rate': 0.0009969653386589748, 'epoch': 0.06}
  6%|▋         | 38/588 [19:17<4:38:14, 30.35s/it]  7%|▋         | 39/588 [19:47<4:37:42, 30.35s/it]                                                  {'loss': 0.7493, 'learning_rate': 0.0009966546331768192, 'epoch': 0.07}
  7%|▋         | 39/588 [19:47<4:37:42, 30.35s/it]  7%|▋         | 40/588 [20:18<4:37:31, 30.39s/it]                                                  {'loss': 0.7176, 'learning_rate': 0.0009963288406760582, 'epoch': 0.07}
  7%|▋         | 40/588 [20:18<4:37:31, 30.39s/it]  7%|▋         | 41/588 [20:48<4:37:06, 30.40s/it]                                                  {'loss': 0.7437, 'learning_rate': 0.0009959879710533834, 'epoch': 0.07}
  7%|▋         | 41/588 [20:48<4:37:06, 30.40s/it]  7%|▋         | 42/588 [21:18<4:35:58, 30.33s/it]                                                  {'loss': 0.793, 'learning_rate': 0.0009956320346634876, 'epoch': 0.07}
  7%|▋         | 42/588 [21:18<4:35:58, 30.33s/it]  7%|▋         | 43/588 [21:49<4:35:37, 30.34s/it]                                                  {'loss': 0.7985, 'learning_rate': 0.0009952610423187517, 'epoch': 0.07}
  7%|▋         | 43/588 [21:49<4:35:37, 30.34s/it]  7%|▋         | 44/588 [22:19<4:35:23, 30.37s/it]                                                  {'loss': 0.7407, 'learning_rate': 0.000994875005288915, 'epoch': 0.07}
  7%|▋         | 44/588 [22:19<4:35:23, 30.37s/it]  8%|▊         | 45/588 [22:49<4:34:49, 30.37s/it]                                                  {'loss': 0.7173, 'learning_rate': 0.0009944739353007343, 'epoch': 0.08}
  8%|▊         | 45/588 [22:49<4:34:49, 30.37s/it]  8%|▊         | 46/588 [23:20<4:34:39, 30.40s/it]                                                  {'loss': 0.7362, 'learning_rate': 0.0009940578445376258, 'epoch': 0.08}
  8%|▊         | 46/588 [23:20<4:34:39, 30.40s/it]  8%|▊         | 47/588 [23:51<4:35:43, 30.58s/it]                                                  {'loss': 0.754, 'learning_rate': 0.0009936267456392971, 'epoch': 0.08}
  8%|▊         | 47/588 [23:51<4:35:43, 30.58s/it]  8%|▊         | 48/588 [24:23<4:40:03, 31.12s/it]                                                  {'loss': 0.7614, 'learning_rate': 0.0009931806517013613, 'epoch': 0.08}
  8%|▊         | 48/588 [24:23<4:40:03, 31.12s/it]  8%|▊         | 49/588 [24:54<4:38:52, 31.04s/it]                                                  {'loss': 0.7376, 'learning_rate': 0.0009927195762749403, 'epoch': 0.08}
  8%|▊         | 49/588 [24:54<4:38:52, 31.04s/it]  9%|▊         | 50/588 [25:25<4:37:47, 30.98s/it]                                                  {'loss': 0.7485, 'learning_rate': 0.0009922435333662536, 'epoch': 0.09}
  9%|▊         | 50/588 [25:25<4:37:47, 30.98s/it]  9%|▊         | 51/588 [25:56<4:36:00, 30.84s/it]                                                  {'loss': 0.7075, 'learning_rate': 0.0009917525374361913, 'epoch': 0.09}
  9%|▊         | 51/588 [25:56<4:36:00, 30.84s/it]  9%|▉         | 52/588 [26:26<4:34:42, 30.75s/it]                                                  {'loss': 0.7258, 'learning_rate': 0.0009912466033998757, 'epoch': 0.09}
  9%|▉         | 52/588 [26:26<4:34:42, 30.75s/it]  9%|▉         | 53/588 [26:57<4:33:25, 30.66s/it]                                                  {'loss': 0.6981, 'learning_rate': 0.000990725746626209, 'epoch': 0.09}
  9%|▉         | 53/588 [26:57<4:33:25, 30.66s/it]  9%|▉         | 54/588 [27:27<4:32:03, 30.57s/it]                                                  {'loss': 0.7345, 'learning_rate': 0.0009901899829374047, 'epoch': 0.09}
  9%|▉         | 54/588 [27:27<4:32:03, 30.57s/it]  9%|▉         | 55/588 [27:57<4:31:03, 30.51s/it]                                                  {'loss': 0.7246, 'learning_rate': 0.0009896393286085083, 'epoch': 0.09}
  9%|▉         | 55/588 [27:57<4:31:03, 30.51s/it] 10%|▉         | 56/588 [28:28<4:31:42, 30.64s/it]                                                  {'loss': 0.7313, 'learning_rate': 0.0009890738003669028, 'epoch': 0.1}
 10%|▉         | 56/588 [28:28<4:31:42, 30.64s/it] 10%|▉         | 57/588 [28:58<4:30:12, 30.53s/it]                                                  {'loss': 0.7524, 'learning_rate': 0.0009884934153917997, 'epoch': 0.1}
 10%|▉         | 57/588 [28:58<4:30:12, 30.53s/it] 10%|▉         | 58/588 [29:29<4:29:22, 30.50s/it]                                                  {'loss': 0.729, 'learning_rate': 0.0009878981913137178, 'epoch': 0.1}
 10%|▉         | 58/588 [29:29<4:29:22, 30.50s/it] 10%|█         | 59/588 [30:00<4:29:58, 30.62s/it]                                                  {'loss': 0.7635, 'learning_rate': 0.0009872881462139479, 'epoch': 0.1}
 10%|█         | 59/588 [30:00<4:29:58, 30.62s/it] 10%|█         | 60/588 [30:31<4:31:14, 30.82s/it]                                                  {'loss': 0.7263, 'learning_rate': 0.000986663298624003, 'epoch': 0.1}
 10%|█         | 60/588 [30:31<4:31:14, 30.82s/it] 10%|█         | 61/588 [31:10<4:52:21, 33.28s/it]                                                  {'loss': 0.667, 'learning_rate': 0.0009860236675250551, 'epoch': 0.1}
 10%|█         | 61/588 [31:10<4:52:21, 33.28s/it] 11%|█         | 62/588 [31:41<4:44:51, 32.49s/it]                                                  {'loss': 0.7389, 'learning_rate': 0.00098536927234736, 'epoch': 0.11}
 11%|█         | 62/588 [31:41<4:44:51, 32.49s/it] 11%|█         | 63/588 [32:11<4:39:13, 31.91s/it]                                                  {'loss': 0.7278, 'learning_rate': 0.0009847001329696653, 'epoch': 0.11}
 11%|█         | 63/588 [32:11<4:39:13, 31.91s/it] 11%|█         | 64/588 [32:42<4:35:11, 31.51s/it]                                                  {'loss': 0.7435, 'learning_rate': 0.0009840162697186076, 'epoch': 0.11}
 11%|█         | 64/588 [32:42<4:35:11, 31.51s/it] 11%|█         | 65/588 [33:12<4:32:16, 31.24s/it]                                                  {'loss': 0.6995, 'learning_rate': 0.0009833177033680945, 'epoch': 0.11}
 11%|█         | 65/588 [33:12<4:32:16, 31.24s/it] 11%|█         | 66/588 [33:43<4:29:46, 31.01s/it]                                                  {'loss': 0.7172, 'learning_rate': 0.0009826044551386743, 'epoch': 0.11}
 11%|█         | 66/588 [33:43<4:29:46, 31.01s/it] 11%|█▏        | 67/588 [34:14<4:28:08, 30.88s/it]                                                  {'loss': 0.7179, 'learning_rate': 0.000981876546696891, 'epoch': 0.11}
 11%|█▏        | 67/588 [34:14<4:28:08, 30.88s/it] 12%|█▏        | 68/588 [34:44<4:27:11, 30.83s/it]                                                  {'loss': 0.7083, 'learning_rate': 0.0009811340001546253, 'epoch': 0.12}
 12%|█▏        | 68/588 [34:44<4:27:11, 30.83s/it] 12%|█▏        | 69/588 [35:15<4:25:35, 30.70s/it]                                                  {'loss': 0.7643, 'learning_rate': 0.0009803768380684242, 'epoch': 0.12}
 12%|█▏        | 69/588 [35:15<4:25:35, 30.70s/it] 12%|█▏        | 70/588 [35:45<4:25:07, 30.71s/it]                                                  {'loss': 0.7234, 'learning_rate': 0.000979605083438815, 'epoch': 0.12}
 12%|█▏        | 70/588 [35:45<4:25:07, 30.71s/it] 12%|█▏        | 71/588 [36:16<4:23:48, 30.62s/it]                                                  {'loss': 0.7051, 'learning_rate': 0.0009788187597096069, 'epoch': 0.12}
 12%|█▏        | 71/588 [36:16<4:23:48, 30.62s/it] 12%|█▏        | 72/588 [36:46<4:22:36, 30.53s/it]                                                  {'loss': 0.7063, 'learning_rate': 0.0009780178907671788, 'epoch': 0.12}
 12%|█▏        | 72/588 [36:46<4:22:36, 30.53s/it] 12%|█▏        | 73/588 [37:17<4:21:44, 30.49s/it]                                                  {'loss': 0.7242, 'learning_rate': 0.0009772025009397538, 'epoch': 0.12}
 12%|█▏        | 73/588 [37:17<4:21:44, 30.49s/it] 13%|█▎        | 74/588 [37:47<4:21:58, 30.58s/it]                                                  {'loss': 0.7029, 'learning_rate': 0.0009763726149966595, 'epoch': 0.13}
 13%|█▎        | 74/588 [37:47<4:21:58, 30.58s/it] 13%|█▎        | 75/588 [38:19<4:23:30, 30.82s/it]                                                  {'loss': 0.6929, 'learning_rate': 0.0009755282581475768, 'epoch': 0.13}
 13%|█▎        | 75/588 [38:19<4:23:30, 30.82s/it] 13%|█▎        | 76/588 [38:50<4:25:06, 31.07s/it]                                                  {'loss': 0.7296, 'learning_rate': 0.0009746694560417731, 'epoch': 0.13}
 13%|█▎        | 76/588 [38:50<4:25:06, 31.07s/it] 13%|█▎        | 77/588 [39:25<4:34:50, 32.27s/it]                                                  {'loss': 0.7112, 'learning_rate': 0.0009737962347673231, 'epoch': 0.13}
 13%|█▎        | 77/588 [39:25<4:34:50, 32.27s/it] 13%|█▎        | 78/588 [39:56<4:29:38, 31.72s/it]                                                  {'loss': 0.7337, 'learning_rate': 0.0009729086208503173, 'epoch': 0.13}
 13%|█▎        | 78/588 [39:56<4:29:38, 31.72s/it] 13%|█▎        | 79/588 [40:26<4:25:45, 31.33s/it]                                                  {'loss': 0.7265, 'learning_rate': 0.0009720066412540554, 'epoch': 0.13}
 13%|█▎        | 79/588 [40:26<4:25:45, 31.33s/it] 14%|█▎        | 80/588 [40:57<4:22:47, 31.04s/it]                                                  {'loss': 0.6896, 'learning_rate': 0.0009710903233782273, 'epoch': 0.14}
 14%|█▎        | 80/588 [40:57<4:22:47, 31.04s/it] 14%|█▍        | 81/588 [41:27<4:21:16, 30.92s/it]                                                  {'loss': 0.7241, 'learning_rate': 0.0009701596950580807, 'epoch': 0.14}
 14%|█▍        | 81/588 [41:27<4:21:16, 30.92s/it] 14%|█▍        | 82/588 [41:58<4:19:47, 30.80s/it]                                                  {'loss': 0.6715, 'learning_rate': 0.000969214784563576, 'epoch': 0.14}
 14%|█▍        | 82/588 [41:58<4:19:47, 30.80s/it] 14%|█▍        | 83/588 [42:29<4:19:14, 30.80s/it]                                                  {'loss': 0.7197, 'learning_rate': 0.0009682556205985273, 'epoch': 0.14}
 14%|█▍        | 83/588 [42:29<4:19:14, 30.80s/it] 14%|█▍        | 84/588 [42:59<4:18:09, 30.73s/it]                                                  {'loss': 0.698, 'learning_rate': 0.0009672822322997304, 'epoch': 0.14}
 14%|█▍        | 84/588 [42:59<4:18:09, 30.73s/it] 14%|█▍        | 85/588 [43:30<4:16:47, 30.63s/it]                                                  {'loss': 0.7106, 'learning_rate': 0.0009662946492360776, 'epoch': 0.14}
 14%|█▍        | 85/588 [43:30<4:16:47, 30.63s/it] 15%|█▍        | 86/588 [44:00<4:15:23, 30.52s/it]                                                  {'loss': 0.7699, 'learning_rate': 0.0009652929014076592, 'epoch': 0.15}
 15%|█▍        | 86/588 [44:00<4:15:23, 30.52s/it] 15%|█▍        | 87/588 [44:31<4:15:23, 30.59s/it]                                                  {'loss': 0.7313, 'learning_rate': 0.0009642770192448536, 'epoch': 0.15}
 15%|█▍        | 87/588 [44:31<4:15:23, 30.59s/it] 15%|█▍        | 88/588 [45:01<4:13:57, 30.48s/it]                                                  {'loss': 0.7434, 'learning_rate': 0.0009632470336074008, 'epoch': 0.15}
 15%|█▍        | 88/588 [45:01<4:13:57, 30.48s/it] 15%|█▌        | 89/588 [45:31<4:13:11, 30.44s/it]                                                  {'loss': 0.722, 'learning_rate': 0.0009622029757834669, 'epoch': 0.15}
 15%|█▌        | 89/588 [45:31<4:13:11, 30.44s/it] 15%|█▌        | 90/588 [46:07<4:25:08, 31.95s/it]                                                  {'loss': 0.7054, 'learning_rate': 0.0009611448774886924, 'epoch': 0.15}
 15%|█▌        | 90/588 [46:07<4:25:08, 31.95s/it] 15%|█▌        | 91/588 [46:39<4:25:40, 32.07s/it]                                                  {'loss': 0.6634, 'learning_rate': 0.0009600727708652289, 'epoch': 0.15}
 15%|█▌        | 91/588 [46:39<4:25:40, 32.07s/it] 16%|█▌        | 92/588 [47:14<4:32:10, 32.93s/it]                                                  {'loss': 0.6921, 'learning_rate': 0.0009589866884807635, 'epoch': 0.16}
 16%|█▌        | 92/588 [47:14<4:32:10, 32.93s/it] 16%|█▌        | 93/588 [47:45<4:25:53, 32.23s/it]                                                  {'loss': 0.6898, 'learning_rate': 0.0009578866633275287, 'epoch': 0.16}
 16%|█▌        | 93/588 [47:45<4:25:53, 32.23s/it] 16%|█▌        | 94/588 [48:15<4:20:50, 31.68s/it]                                                  {'loss': 0.7051, 'learning_rate': 0.0009567727288213005, 'epoch': 0.16}
 16%|█▌        | 94/588 [48:15<4:20:50, 31.68s/it] 16%|█▌        | 95/588 [48:45<4:17:03, 31.28s/it]                                                  {'loss': 0.7001, 'learning_rate': 0.0009556449188003831, 'epoch': 0.16}
 16%|█▌        | 95/588 [48:45<4:17:03, 31.28s/it] 16%|█▋        | 96/588 [49:16<4:14:13, 31.00s/it]                                                  {'loss': 0.7029, 'learning_rate': 0.0009545032675245813, 'epoch': 0.16}
 16%|█▋        | 96/588 [49:16<4:14:13, 31.00s/it] 16%|█▋        | 97/588 [49:46<4:12:29, 30.85s/it]                                                  {'loss': 0.6678, 'learning_rate': 0.0009533478096741596, 'epoch': 0.16}
 16%|█▋        | 97/588 [49:46<4:12:29, 30.85s/it] 17%|█▋        | 98/588 [50:17<4:10:54, 30.72s/it]                                                  {'loss': 0.7366, 'learning_rate': 0.0009521785803487888, 'epoch': 0.17}
 17%|█▋        | 98/588 [50:17<4:10:54, 30.72s/it] 17%|█▋        | 99/588 [50:47<4:09:36, 30.63s/it]                                                  {'loss': 0.7043, 'learning_rate': 0.0009509956150664796, 'epoch': 0.17}
 17%|█▋        | 99/588 [50:47<4:09:36, 30.63s/it] 17%|█▋        | 100/588 [51:17<4:08:54, 30.60s/it]                                                   {'loss': 0.6667, 'learning_rate': 0.0009497989497625035, 'epoch': 0.17}
 17%|█▋        | 100/588 [51:17<4:08:54, 30.60s/it] 17%|█▋        | 101/588 [51:48<4:07:38, 30.51s/it]                                                   {'loss': 0.6784, 'learning_rate': 0.0009485886207883022, 'epoch': 0.17}
 17%|█▋        | 101/588 [51:48<4:07:38, 30.51s/it] 17%|█▋        | 102/588 [52:28<4:29:35, 33.28s/it]                                                   {'loss': 0.6696, 'learning_rate': 0.0009473646649103818, 'epoch': 0.17}
 17%|█▋        | 102/588 [52:28<4:29:35, 33.28s/it] 18%|█▊        | 103/588 [53:15<5:03:56, 37.60s/it]                                                   {'loss': 0.6878, 'learning_rate': 0.000946127119309197, 'epoch': 0.18}
 18%|█▊        | 103/588 [53:15<5:03:56, 37.60s/it] 18%|█▊        | 104/588 [53:46<4:46:17, 35.49s/it]                                                   {'loss': 0.6869, 'learning_rate': 0.0009448760215780217, 'epoch': 0.18}
 18%|█▊        | 104/588 [53:46<4:46:17, 35.49s/it] 18%|█▊        | 105/588 [54:16<4:33:29, 33.97s/it]                                                   {'loss': 0.7351, 'learning_rate': 0.000943611409721806, 'epoch': 0.18}
 18%|█▊        | 105/588 [54:16<4:33:29, 33.97s/it] 18%|█▊        | 106/588 [54:47<4:24:17, 32.90s/it]                                                   {'loss': 0.6918, 'learning_rate': 0.000942333322156023, 'epoch': 0.18}
 18%|█▊        | 106/588 [54:47<4:24:17, 32.90s/it] 18%|█▊        | 107/588 [55:17<4:17:56, 32.18s/it]                                                   {'loss': 0.6835, 'learning_rate': 0.000941041797705501, 'epoch': 0.18}
 18%|█▊        | 107/588 [55:17<4:17:56, 32.18s/it] 18%|█▊        | 108/588 [55:47<4:12:54, 31.61s/it]                                                   {'loss': 0.7069, 'learning_rate': 0.0009397368756032445, 'epoch': 0.18}
 18%|█▊        | 108/588 [55:47<4:12:54, 31.61s/it] 19%|█▊        | 109/588 [56:18<4:09:21, 31.23s/it]                                                   {'loss': 0.6491, 'learning_rate': 0.0009384185954892422, 'epoch': 0.19}
 19%|█▊        | 109/588 [56:18<4:09:21, 31.23s/it] 19%|█▊        | 110/588 [56:48<4:06:23, 30.93s/it]                                                   {'loss': 0.752, 'learning_rate': 0.0009370869974092629, 'epoch': 0.19}
 19%|█▊        | 110/588 [56:48<4:06:23, 30.93s/it] 19%|█▉        | 111/588 [57:18<4:04:28, 30.75s/it]                                                   {'loss': 0.7039, 'learning_rate': 0.0009357421218136386, 'epoch': 0.19}
 19%|█▉        | 111/588 [57:18<4:04:28, 30.75s/it] 19%|█▉        | 112/588 [57:49<4:03:13, 30.66s/it]                                                   {'loss': 0.6714, 'learning_rate': 0.0009343840095560372, 'epoch': 0.19}
 19%|█▉        | 112/588 [57:49<4:03:13, 30.66s/it] 19%|█▉        | 113/588 [58:20<4:04:28, 30.88s/it]                                                   {'loss': 0.6986, 'learning_rate': 0.0009330127018922195, 'epoch': 0.19}
 19%|█▉        | 113/588 [58:20<4:04:28, 30.88s/it] 19%|█▉        | 114/588 [58:56<4:16:25, 32.46s/it]                                                   {'loss': 0.6882, 'learning_rate': 0.000931628240478787, 'epoch': 0.19}
 19%|█▉        | 114/588 [58:56<4:16:25, 32.46s/it] 20%|█▉        | 115/588 [59:31<4:21:16, 33.14s/it]                                                   {'loss': 0.6733, 'learning_rate': 0.0009302306673719168, 'epoch': 0.2}
 20%|█▉        | 115/588 [59:31<4:21:16, 33.14s/it] 20%|█▉        | 116/588 [1:00:01<4:14:27, 32.35s/it]                                                     {'loss': 0.6863, 'learning_rate': 0.0009288200250260835, 'epoch': 0.2}
 20%|█▉        | 116/588 [1:00:02<4:14:27, 32.35s/it] 20%|█▉        | 117/588 [1:00:32<4:09:20, 31.76s/it]                                                     {'loss': 0.6501, 'learning_rate': 0.0009273963562927695, 'epoch': 0.2}
 20%|█▉        | 117/588 [1:00:32<4:09:20, 31.76s/it] 20%|██        | 118/588 [1:01:02<4:05:42, 31.37s/it]                                                     {'loss': 0.6816, 'learning_rate': 0.0009259597044191636, 'epoch': 0.2}
 20%|██        | 118/588 [1:01:02<4:05:42, 31.37s/it] 20%|██        | 119/588 [1:01:33<4:03:03, 31.09s/it]                                                     {'loss': 0.6993, 'learning_rate': 0.000924510113046847, 'epoch': 0.2}
 20%|██        | 119/588 [1:01:33<4:03:03, 31.09s/it] 20%|██        | 120/588 [1:02:03<4:01:08, 30.92s/it]                                                     {'loss': 0.6969, 'learning_rate': 0.0009230476262104677, 'epoch': 0.2}
 20%|██        | 120/588 [1:02:03<4:01:08, 30.92s/it] 21%|██        | 121/588 [1:02:34<3:59:43, 30.80s/it]                                                     {'loss': 0.6974, 'learning_rate': 0.0009215722883364033, 'epoch': 0.21}
 21%|██        | 121/588 [1:02:34<3:59:43, 30.80s/it] 21%|██        | 122/588 [1:03:05<3:59:18, 30.81s/it]                                                     {'loss': 0.6626, 'learning_rate': 0.0009200841442414105, 'epoch': 0.21}
 21%|██        | 122/588 [1:03:05<3:59:18, 30.81s/it] 21%|██        | 123/588 [1:03:35<3:57:28, 30.64s/it]                                                     {'loss': 0.714, 'learning_rate': 0.0009185832391312643, 'epoch': 0.21}
 21%|██        | 123/588 [1:03:35<3:57:28, 30.64s/it] 21%|██        | 124/588 [1:04:05<3:56:37, 30.60s/it]                                                     {'loss': 0.7057, 'learning_rate': 0.000917069618599385, 'epoch': 0.21}
 21%|██        | 124/588 [1:04:05<3:56:37, 30.60s/it] 21%|██▏       | 125/588 [1:04:36<3:55:30, 30.52s/it]                                                     {'loss': 0.6711, 'learning_rate': 0.0009155433286254524, 'epoch': 0.21}
 21%|██▏       | 125/588 [1:04:36<3:55:30, 30.52s/it] 21%|██▏       | 126/588 [1:05:06<3:53:34, 30.33s/it]                                                     {'loss': 0.73, 'learning_rate': 0.00091400441557401, 'epoch': 0.21}
 21%|██▏       | 126/588 [1:05:06<3:53:34, 30.33s/it] 22%|██▏       | 127/588 [1:05:36<3:52:46, 30.30s/it]                                                     {'loss': 0.7159, 'learning_rate': 0.0009124529261930559, 'epoch': 0.22}
 22%|██▏       | 127/588 [1:05:36<3:52:46, 30.30s/it] 22%|██▏       | 128/588 [1:06:06<3:52:22, 30.31s/it]                                                     {'loss': 0.6935, 'learning_rate': 0.0009108889076126225, 'epoch': 0.22}
 22%|██▏       | 128/588 [1:06:06<3:52:22, 30.31s/it] 22%|██▏       | 129/588 [1:06:37<3:52:07, 30.34s/it]                                                     {'loss': 0.7193, 'learning_rate': 0.0009093124073433463, 'epoch': 0.22}
 22%|██▏       | 129/588 [1:06:37<3:52:07, 30.34s/it] 22%|██▏       | 130/588 [1:07:07<3:51:41, 30.35s/it]                                                     {'loss': 0.7039, 'learning_rate': 0.0009077234732750224, 'epoch': 0.22}
 22%|██▏       | 130/588 [1:07:07<3:51:41, 30.35s/it] 22%|██▏       | 131/588 [1:07:38<3:53:47, 30.70s/it]                                                     {'loss': 0.699, 'learning_rate': 0.0009061221536751517, 'epoch': 0.22}
 22%|██▏       | 131/588 [1:07:38<3:53:47, 30.70s/it] 22%|██▏       | 132/588 [1:08:09<3:52:49, 30.63s/it]                                                     {'loss': 0.6804, 'learning_rate': 0.0009045084971874737, 'epoch': 0.22}
 22%|██▏       | 132/588 [1:08:09<3:52:49, 30.63s/it] 23%|██▎       | 133/588 [1:08:39<3:51:42, 30.55s/it]                                                     {'loss': 0.7341, 'learning_rate': 0.0009028825528304891, 'epoch': 0.23}
 23%|██▎       | 133/588 [1:08:39<3:51:42, 30.55s/it] 23%|██▎       | 134/588 [1:09:10<3:50:45, 30.50s/it]                                                     {'loss': 0.6702, 'learning_rate': 0.0009012443699959704, 'epoch': 0.23}
 23%|██▎       | 134/588 [1:09:10<3:50:45, 30.50s/it] 23%|██▎       | 135/588 [1:09:40<3:50:01, 30.47s/it]                                                     {'loss': 0.6951, 'learning_rate': 0.0008995939984474624, 'epoch': 0.23}
 23%|██▎       | 135/588 [1:09:40<3:50:01, 30.47s/it] 23%|██▎       | 136/588 [1:10:10<3:48:48, 30.37s/it]                                                     {'loss': 0.7206, 'learning_rate': 0.0008979314883187692, 'epoch': 0.23}
 23%|██▎       | 136/588 [1:10:10<3:48:48, 30.37s/it] 23%|██▎       | 137/588 [1:10:41<3:48:12, 30.36s/it]                                                     {'loss': 0.6782, 'learning_rate': 0.0008962568901124327, 'epoch': 0.23}
 23%|██▎       | 137/588 [1:10:41<3:48:12, 30.36s/it] 23%|██▎       | 138/588 [1:11:11<3:48:04, 30.41s/it]                                                     {'loss': 0.6986, 'learning_rate': 0.0008945702546981969, 'epoch': 0.23}
 23%|██▎       | 138/588 [1:11:11<3:48:04, 30.41s/it] 24%|██▎       | 139/588 [1:11:42<3:47:49, 30.44s/it]                                                     {'loss': 0.6984, 'learning_rate': 0.0008928716333114643, 'epoch': 0.24}
 24%|██▎       | 139/588 [1:11:42<3:47:49, 30.44s/it] 24%|██▍       | 140/588 [1:12:12<3:47:27, 30.46s/it]                                                     {'loss': 0.6609, 'learning_rate': 0.0008911610775517383, 'epoch': 0.24}
 24%|██▍       | 140/588 [1:12:12<3:47:27, 30.46s/it] 24%|██▍       | 141/588 [1:12:47<3:56:43, 31.77s/it]                                                     {'loss': 0.6846, 'learning_rate': 0.0008894386393810563, 'epoch': 0.24}
 24%|██▍       | 141/588 [1:12:47<3:56:43, 31.77s/it] 24%|██▍       | 142/588 [1:13:17<3:51:13, 31.11s/it]                                                     {'loss': 0.6959, 'learning_rate': 0.0008877043711224108, 'epoch': 0.24}
 24%|██▍       | 142/588 [1:13:17<3:51:13, 31.11s/it] 24%|██▍       | 143/588 [1:13:47<3:49:42, 30.97s/it]                                                     {'loss': 0.6721, 'learning_rate': 0.0008859583254581605, 'epoch': 0.24}
 24%|██▍       | 143/588 [1:13:47<3:49:42, 30.97s/it] 24%|██▍       | 144/588 [1:14:18<3:47:53, 30.80s/it]                                                     {'loss': 0.6473, 'learning_rate': 0.0008842005554284296, 'epoch': 0.24}
 24%|██▍       | 144/588 [1:14:18<3:47:53, 30.80s/it] 25%|██▍       | 145/588 [1:14:48<3:46:39, 30.70s/it]                                                     {'loss': 0.6881, 'learning_rate': 0.0008824311144294965, 'epoch': 0.25}
 25%|██▍       | 145/588 [1:14:48<3:46:39, 30.70s/it] 25%|██▍       | 146/588 [1:15:18<3:45:17, 30.58s/it]                                                     {'loss': 0.6964, 'learning_rate': 0.0008806500562121724, 'epoch': 0.25}
 25%|██▍       | 146/588 [1:15:18<3:45:17, 30.58s/it] 25%|██▌       | 147/588 [1:15:49<3:44:29, 30.54s/it]                                                     {'loss': 0.6678, 'learning_rate': 0.0008788574348801675, 'epoch': 0.25}
 25%|██▌       | 147/588 [1:15:49<3:44:29, 30.54s/it] 25%|██▌       | 148/588 [1:16:19<3:43:41, 30.50s/it]                                                     {'loss': 0.6621, 'learning_rate': 0.0008770533048884482, 'epoch': 0.25}
 25%|██▌       | 148/588 [1:16:19<3:43:41, 30.50s/it] 25%|██▌       | 149/588 [1:16:50<3:43:15, 30.51s/it]                                                     {'loss': 0.661, 'learning_rate': 0.000875237721041583, 'epoch': 0.25}
 25%|██▌       | 149/588 [1:16:50<3:43:15, 30.51s/it] 26%|██▌       | 150/588 [1:17:20<3:42:26, 30.47s/it]                                                     {'loss': 0.696, 'learning_rate': 0.000873410738492077, 'epoch': 0.26}
 26%|██▌       | 150/588 [1:17:20<3:42:26, 30.47s/it] 26%|██▌       | 151/588 [1:17:51<3:41:47, 30.45s/it]                                                     {'loss': 0.6727, 'learning_rate': 0.0008715724127386971, 'epoch': 0.26}
 26%|██▌       | 151/588 [1:17:51<3:41:47, 30.45s/it] 26%|██▌       | 152/588 [1:18:21<3:40:54, 30.40s/it]                                                     {'loss': 0.6923, 'learning_rate': 0.0008697227996247861, 'epoch': 0.26}
 26%|██▌       | 152/588 [1:18:21<3:40:54, 30.40s/it] 26%|██▌       | 153/588 [1:18:51<3:40:26, 30.41s/it]                                                     {'loss': 0.6746, 'learning_rate': 0.0008678619553365659, 'epoch': 0.26}
 26%|██▌       | 153/588 [1:18:51<3:40:26, 30.41s/it] 26%|██▌       | 154/588 [1:19:22<3:40:06, 30.43s/it]                                                     {'loss': 0.6297, 'learning_rate': 0.0008659899364014308, 'epoch': 0.26}
 26%|██▌       | 154/588 [1:19:22<3:40:06, 30.43s/it] 26%|██▋       | 155/588 [1:19:52<3:39:42, 30.45s/it]                                                     {'loss': 0.6701, 'learning_rate': 0.0008641067996862311, 'epoch': 0.26}
 26%|██▋       | 155/588 [1:19:52<3:39:42, 30.45s/it] 27%|██▋       | 156/588 [1:20:23<3:39:10, 30.44s/it]                                                     {'loss': 0.6576, 'learning_rate': 0.0008622126023955446, 'epoch': 0.27}
 27%|██▋       | 156/588 [1:20:23<3:39:10, 30.44s/it] 27%|██▋       | 157/588 [1:20:53<3:38:33, 30.43s/it]                                                     {'loss': 0.6145, 'learning_rate': 0.0008603074020699392, 'epoch': 0.27}
 27%|██▋       | 157/588 [1:20:53<3:38:33, 30.43s/it] 27%|██▋       | 158/588 [1:21:23<3:37:41, 30.38s/it]                                                     {'loss': 0.7037, 'learning_rate': 0.0008583912565842257, 'epoch': 0.27}
 27%|██▋       | 158/588 [1:21:23<3:37:41, 30.38s/it] 27%|██▋       | 159/588 [1:21:54<3:37:24, 30.41s/it]                                                     {'loss': 0.642, 'learning_rate': 0.0008564642241456986, 'epoch': 0.27}
 27%|██▋       | 159/588 [1:21:54<3:37:24, 30.41s/it] 27%|██▋       | 160/588 [1:22:24<3:37:35, 30.50s/it]                                                     {'loss': 0.65, 'learning_rate': 0.0008545263632923686, 'epoch': 0.27}
 27%|██▋       | 160/588 [1:22:25<3:37:35, 30.50s/it] 27%|██▋       | 161/588 [1:22:55<3:37:03, 30.50s/it]                                                     {'loss': 0.6842, 'learning_rate': 0.0008525777328911846, 'epoch': 0.27}
 27%|██▋       | 161/588 [1:22:55<3:37:03, 30.50s/it] 28%|██▊       | 162/588 [1:23:25<3:35:57, 30.42s/it]                                                     {'loss': 0.6796, 'learning_rate': 0.0008506183921362443, 'epoch': 0.28}
 28%|██▊       | 162/588 [1:23:25<3:35:57, 30.42s/it] 28%|██▊       | 163/588 [1:23:56<3:35:24, 30.41s/it]                                                     {'loss': 0.6411, 'learning_rate': 0.0008486484005469976, 'epoch': 0.28}
 28%|██▊       | 163/588 [1:23:56<3:35:24, 30.41s/it] 28%|██▊       | 164/588 [1:24:40<4:04:18, 34.57s/it]                                                     {'loss': 0.6801, 'learning_rate': 0.0008466678179664378, 'epoch': 0.28}
 28%|██▊       | 164/588 [1:24:40<4:04:18, 34.57s/it] 28%|██▊       | 165/588 [1:25:10<3:55:13, 33.36s/it]                                                     {'loss': 0.6526, 'learning_rate': 0.0008446767045592829, 'epoch': 0.28}
 28%|██▊       | 165/588 [1:25:10<3:55:13, 33.36s/it] 28%|██▊       | 166/588 [1:25:41<3:48:13, 32.45s/it]                                                     {'loss': 0.6676, 'learning_rate': 0.00084267512081015, 'epoch': 0.28}
 28%|██▊       | 166/588 [1:25:41<3:48:13, 32.45s/it] 28%|██▊       | 167/588 [1:26:11<3:43:05, 31.79s/it]                                                     {'loss': 0.6799, 'learning_rate': 0.0008406631275217156, 'epoch': 0.28}
 28%|██▊       | 167/588 [1:26:11<3:43:05, 31.79s/it] 29%|██▊       | 168/588 [1:26:42<3:39:50, 31.41s/it]                                                     {'loss': 0.6349, 'learning_rate': 0.0008386407858128706, 'epoch': 0.29}
 29%|██▊       | 168/588 [1:26:42<3:39:50, 31.41s/it] 29%|██▊       | 169/588 [1:27:20<3:53:23, 33.42s/it]                                                     {'loss': 0.6826, 'learning_rate': 0.0008366081571168625, 'epoch': 0.29}
 29%|██▊       | 169/588 [1:27:20<3:53:23, 33.42s/it] 29%|██▉       | 170/588 [1:27:50<3:46:35, 32.53s/it]                                                     {'loss': 0.6947, 'learning_rate': 0.0008345653031794292, 'epoch': 0.29}
 29%|██▉       | 170/588 [1:27:50<3:46:35, 32.53s/it] 29%|██▉       | 171/588 [1:28:25<3:50:24, 33.15s/it]                                                     {'loss': 0.6547, 'learning_rate': 0.0008325122860569241, 'epoch': 0.29}
 29%|██▉       | 171/588 [1:28:25<3:50:24, 33.15s/it] 29%|██▉       | 172/588 [1:28:55<3:44:12, 32.34s/it]                                                     {'loss': 0.6688, 'learning_rate': 0.0008304491681144306, 'epoch': 0.29}
 29%|██▉       | 172/588 [1:28:55<3:44:12, 32.34s/it] 29%|██▉       | 173/588 [1:29:25<3:39:24, 31.72s/it]                                                     {'loss': 0.6875, 'learning_rate': 0.0008283760120238673, 'epoch': 0.29}
 29%|██▉       | 173/588 [1:29:25<3:39:24, 31.72s/it] 30%|██▉       | 174/588 [1:29:56<3:36:07, 31.32s/it]                                                     {'loss': 0.6708, 'learning_rate': 0.0008262928807620843, 'epoch': 0.3}
 30%|██▉       | 174/588 [1:29:56<3:36:07, 31.32s/it] 30%|██▉       | 175/588 [1:30:26<3:34:04, 31.10s/it]                                                     {'loss': 0.6374, 'learning_rate': 0.0008241998376089508, 'epoch': 0.3}
 30%|██▉       | 175/588 [1:30:26<3:34:04, 31.10s/it] 30%|██▉       | 176/588 [1:30:57<3:32:12, 30.90s/it]                                                     {'loss': 0.691, 'learning_rate': 0.0008220969461454322, 'epoch': 0.3}
 30%|██▉       | 176/588 [1:30:57<3:32:12, 30.90s/it] 30%|███       | 177/588 [1:31:27<3:30:24, 30.72s/it]                                                     {'loss': 0.6522, 'learning_rate': 0.0008199842702516583, 'epoch': 0.3}
 30%|███       | 177/588 [1:31:27<3:30:24, 30.72s/it] 30%|███       | 178/588 [1:31:58<3:29:16, 30.63s/it]                                                     {'loss': 0.676, 'learning_rate': 0.0008178618741049842, 'epoch': 0.3}
 30%|███       | 178/588 [1:31:58<3:29:16, 30.63s/it] 30%|███       | 179/588 [1:32:28<3:28:14, 30.55s/it]                                                     {'loss': 0.658, 'learning_rate': 0.0008157298221780388, 'epoch': 0.3}
 30%|███       | 179/588 [1:32:28<3:28:14, 30.55s/it] 31%|███       | 180/588 [1:32:58<3:26:59, 30.44s/it]                                                     {'loss': 0.6515, 'learning_rate': 0.0008135881792367685, 'epoch': 0.31}
 31%|███       | 180/588 [1:32:58<3:26:59, 30.44s/it] 31%|███       | 181/588 [1:33:28<3:26:18, 30.41s/it]                                                     {'loss': 0.6526, 'learning_rate': 0.0008114370103384681, 'epoch': 0.31}
 31%|███       | 181/588 [1:33:28<3:26:18, 30.41s/it] 31%|███       | 182/588 [1:33:59<3:25:47, 30.41s/it]                                                     {'loss': 0.6176, 'learning_rate': 0.0008092763808298047, 'epoch': 0.31}
 31%|███       | 182/588 [1:33:59<3:25:47, 30.41s/it] 31%|███       | 183/588 [1:34:29<3:25:05, 30.38s/it]                                                     {'loss': 0.6355, 'learning_rate': 0.000807106356344834, 'epoch': 0.31}
 31%|███       | 183/588 [1:34:29<3:25:05, 30.38s/it] 31%|███▏      | 184/588 [1:35:00<3:24:44, 30.41s/it]                                                     {'loss': 0.6941, 'learning_rate': 0.0008049270028030046, 'epoch': 0.31}
 31%|███▏      | 184/588 [1:35:00<3:24:44, 30.41s/it] 31%|███▏      | 185/588 [1:35:32<3:28:13, 31.00s/it]                                                     {'loss': 0.6453, 'learning_rate': 0.0008027383864071572, 'epoch': 0.31}
 31%|███▏      | 185/588 [1:35:32<3:28:13, 31.00s/it] 32%|███▏      | 186/588 [1:36:05<3:30:45, 31.46s/it]                                                     {'loss': 0.6755, 'learning_rate': 0.0008005405736415125, 'epoch': 0.32}
 32%|███▏      | 186/588 [1:36:05<3:30:45, 31.46s/it] 32%|███▏      | 187/588 [1:36:44<3:46:02, 33.82s/it]                                                     {'loss': 0.6858, 'learning_rate': 0.000798333631269652, 'epoch': 0.32}
 32%|███▏      | 187/588 [1:36:44<3:46:02, 33.82s/it] 32%|███▏      | 188/588 [1:37:14<3:38:49, 32.82s/it]                                                     {'loss': 0.6835, 'learning_rate': 0.00079611762633249, 'epoch': 0.32}
 32%|███▏      | 188/588 [1:37:14<3:38:49, 32.82s/it] 32%|███▏      | 189/588 [1:37:45<3:33:27, 32.10s/it]                                                     {'loss': 0.6769, 'learning_rate': 0.0007938926261462366, 'epoch': 0.32}
 32%|███▏      | 189/588 [1:37:45<3:33:27, 32.10s/it] 32%|███▏      | 190/588 [1:38:15<3:29:50, 31.63s/it]                                                     {'loss': 0.6511, 'learning_rate': 0.0007916586983003533, 'epoch': 0.32}
 32%|███▏      | 190/588 [1:38:15<3:29:50, 31.63s/it] 32%|███▏      | 191/588 [1:38:46<3:26:52, 31.27s/it]                                                     {'loss': 0.668, 'learning_rate': 0.0007894159106554997, 'epoch': 0.32}
 32%|███▏      | 191/588 [1:38:46<3:26:52, 31.27s/it] 33%|███▎      | 192/588 [1:39:16<3:24:27, 30.98s/it]                                                     {'loss': 0.6563, 'learning_rate': 0.0007871643313414718, 'epoch': 0.33}
 33%|███▎      | 192/588 [1:39:16<3:24:27, 30.98s/it] 33%|███▎      | 193/588 [1:39:47<3:23:39, 30.94s/it]                                                     {'loss': 0.6283, 'learning_rate': 0.0007849040287551332, 'epoch': 0.33}
 33%|███▎      | 193/588 [1:39:47<3:23:39, 30.94s/it] 33%|███▎      | 194/588 [1:40:21<3:28:31, 31.76s/it]                                                     {'loss': 0.6247, 'learning_rate': 0.0007826350715583359, 'epoch': 0.33}
 33%|███▎      | 194/588 [1:40:21<3:28:31, 31.76s/it] 33%|███▎      | 195/588 [1:40:52<3:26:56, 31.60s/it]                                                     {'loss': 0.6224, 'learning_rate': 0.0007803575286758365, 'epoch': 0.33}
 33%|███▎      | 195/588 [1:40:52<3:26:56, 31.60s/it] 33%|███▎      | 196/588 [1:41:22<3:24:11, 31.25s/it]                                                     {'loss': 0.6499, 'learning_rate': 0.0007780714692932003, 'epoch': 0.33}
 33%|███▎      | 196/588 [1:41:22<3:24:11, 31.25s/it] 34%|███▎      | 197/588 [1:41:59<3:34:22, 32.90s/it]                                                     {'loss': 0.6348, 'learning_rate': 0.0007757769628547018, 'epoch': 0.34}
 34%|███▎      | 197/588 [1:41:59<3:34:22, 32.90s/it] 34%|███▎      | 198/588 [1:42:29<3:28:55, 32.14s/it]                                                     {'loss': 0.6776, 'learning_rate': 0.0007734740790612135, 'epoch': 0.34}
 34%|███▎      | 198/588 [1:42:29<3:28:55, 32.14s/it] 34%|███▍      | 199/588 [1:43:00<3:25:15, 31.66s/it]                                                     {'loss': 0.648, 'learning_rate': 0.0007711628878680892, 'epoch': 0.34}
 34%|███▍      | 199/588 [1:43:00<3:25:15, 31.66s/it] 34%|███▍      | 200/588 [1:43:31<3:23:22, 31.45s/it]                                                     {'loss': 0.6526, 'learning_rate': 0.0007688434594830391, 'epoch': 0.34}
 34%|███▍      | 200/588 [1:43:31<3:23:22, 31.45s/it] 34%|███▍      | 201/588 [1:44:01<3:20:44, 31.12s/it]                                                     {'loss': 0.6479, 'learning_rate': 0.0007665158643639969, 'epoch': 0.34}
 34%|███▍      | 201/588 [1:44:01<3:20:44, 31.12s/it] 34%|███▍      | 202/588 [1:44:32<3:18:50, 30.91s/it]                                                     {'loss': 0.6467, 'learning_rate': 0.0007641801732169795, 'epoch': 0.34}
 34%|███▍      | 202/588 [1:44:32<3:18:50, 30.91s/it] 35%|███▍      | 203/588 [1:45:09<3:31:42, 32.99s/it]                                                     {'loss': 0.6294, 'learning_rate': 0.000761836456993939, 'epoch': 0.35}
 35%|███▍      | 203/588 [1:45:09<3:31:42, 32.99s/it] 35%|███▍      | 204/588 [1:45:39<3:25:08, 32.05s/it]                                                     {'loss': 0.664, 'learning_rate': 0.0007594847868906076, 'epoch': 0.35}
 35%|███▍      | 204/588 [1:45:39<3:25:08, 32.05s/it] 35%|███▍      | 205/588 [1:46:10<3:21:38, 31.59s/it]                                                     {'loss': 0.6431, 'learning_rate': 0.0007571252343443349, 'epoch': 0.35}
 35%|███▍      | 205/588 [1:46:10<3:21:38, 31.59s/it] 35%|███▌      | 206/588 [1:46:40<3:18:28, 31.17s/it]                                                     {'loss': 0.6821, 'learning_rate': 0.0007547578710319174, 'epoch': 0.35}
 35%|███▌      | 206/588 [1:46:40<3:18:28, 31.17s/it] 35%|███▌      | 207/588 [1:47:16<3:26:22, 32.50s/it]                                                     {'loss': 0.6664, 'learning_rate': 0.000752382768867422, 'epoch': 0.35}
 35%|███▌      | 207/588 [1:47:16<3:26:22, 32.50s/it] 35%|███▌      | 208/588 [1:47:46<3:21:53, 31.88s/it]                                                     {'loss': 0.6748, 'learning_rate': 0.00075, 'epoch': 0.35}
 35%|███▌      | 208/588 [1:47:46<3:21:53, 31.88s/it] 36%|███▌      | 209/588 [1:48:16<3:18:15, 31.39s/it]                                                     {'loss': 0.6784, 'learning_rate': 0.0007476096368116973, 'epoch': 0.36}
 36%|███▌      | 209/588 [1:48:16<3:18:15, 31.39s/it] 36%|███▌      | 210/588 [1:48:47<3:15:59, 31.11s/it]                                                     {'loss': 0.6562, 'learning_rate': 0.0007452117519152541, 'epoch': 0.36}
 36%|███▌      | 210/588 [1:48:47<3:15:59, 31.11s/it] 36%|███▌      | 211/588 [1:49:17<3:14:10, 30.90s/it]                                                     {'loss': 0.6254, 'learning_rate': 0.0007428064181518997, 'epoch': 0.36}
 36%|███▌      | 211/588 [1:49:17<3:14:10, 30.90s/it] 36%|███▌      | 212/588 [1:49:48<3:13:00, 30.80s/it]                                                     {'loss': 0.7151, 'learning_rate': 0.0007403937085891397, 'epoch': 0.36}
 36%|███▌      | 212/588 [1:49:48<3:13:00, 30.80s/it] 36%|███▌      | 213/588 [1:50:18<3:11:41, 30.67s/it]                                                     {'loss': 0.6266, 'learning_rate': 0.0007379736965185368, 'epoch': 0.36}
 36%|███▌      | 213/588 [1:50:18<3:11:41, 30.67s/it] 36%|███▋      | 214/588 [1:50:48<3:10:38, 30.58s/it]                                                     {'loss': 0.6601, 'learning_rate': 0.0007355464554534836, 'epoch': 0.36}
 36%|███▋      | 214/588 [1:50:48<3:10:38, 30.58s/it] 37%|███▋      | 215/588 [1:51:19<3:10:23, 30.63s/it]                                                     {'loss': 0.6661, 'learning_rate': 0.0007331120591269701, 'epoch': 0.37}
 37%|███▋      | 215/588 [1:51:19<3:10:23, 30.63s/it] 37%|███▋      | 216/588 [1:51:52<3:14:36, 31.39s/it]                                                     {'loss': 0.5952, 'learning_rate': 0.000730670581489344, 'epoch': 0.37}
 37%|███▋      | 216/588 [1:51:52<3:14:36, 31.39s/it] 37%|███▋      | 217/588 [1:52:23<3:12:21, 31.11s/it]                                                     {'loss': 0.6399, 'learning_rate': 0.0007282220967060633, 'epoch': 0.37}
 37%|███▋      | 217/588 [1:52:23<3:12:21, 31.11s/it] 37%|███▋      | 218/588 [1:52:53<3:10:19, 30.86s/it]                                                     {'loss': 0.6298, 'learning_rate': 0.0007257666791554447, 'epoch': 0.37}
 37%|███▋      | 218/588 [1:52:53<3:10:19, 30.86s/it] 37%|███▋      | 219/588 [1:53:23<3:08:48, 30.70s/it]                                                     {'loss': 0.6586, 'learning_rate': 0.0007233044034264033, 'epoch': 0.37}
 37%|███▋      | 219/588 [1:53:23<3:08:48, 30.70s/it] 37%|███▋      | 220/588 [1:53:54<3:07:53, 30.64s/it]                                                     {'loss': 0.633, 'learning_rate': 0.0007208353443161871, 'epoch': 0.37}
 37%|███▋      | 220/588 [1:53:54<3:07:53, 30.64s/it] 38%|███▊      | 221/588 [1:54:24<3:06:40, 30.52s/it]                                                     {'loss': 0.6565, 'learning_rate': 0.0007183595768281044, 'epoch': 0.38}
 38%|███▊      | 221/588 [1:54:24<3:06:40, 30.52s/it] 38%|███▊      | 222/588 [1:54:55<3:05:51, 30.47s/it]                                                     {'loss': 0.651, 'learning_rate': 0.0007158771761692464, 'epoch': 0.38}
 38%|███▊      | 222/588 [1:54:55<3:05:51, 30.47s/it] 38%|███▊      | 223/588 [1:55:25<3:05:14, 30.45s/it]                                                     {'loss': 0.6646, 'learning_rate': 0.0007133882177482019, 'epoch': 0.38}
 38%|███▊      | 223/588 [1:55:25<3:05:14, 30.45s/it] 38%|███▊      | 224/588 [1:55:55<3:04:44, 30.45s/it]                                                     {'loss': 0.6297, 'learning_rate': 0.0007108927771727661, 'epoch': 0.38}
 38%|███▊      | 224/588 [1:55:55<3:04:44, 30.45s/it] 38%|███▊      | 225/588 [1:56:28<3:07:26, 30.98s/it]                                                     {'loss': 0.6479, 'learning_rate': 0.0007083909302476452, 'epoch': 0.38}
 38%|███▊      | 225/588 [1:56:28<3:07:26, 30.98s/it] 38%|███▊      | 226/588 [1:56:58<3:06:08, 30.85s/it]                                                     {'loss': 0.6362, 'learning_rate': 0.0007058827529721525, 'epoch': 0.38}
 38%|███▊      | 226/588 [1:56:58<3:06:08, 30.85s/it] 39%|███▊      | 227/588 [1:57:29<3:04:46, 30.71s/it]                                                     {'loss': 0.6542, 'learning_rate': 0.0007033683215379002, 'epoch': 0.39}
 39%|███▊      | 227/588 [1:57:29<3:04:46, 30.71s/it] 39%|███▉      | 228/588 [1:57:59<3:03:42, 30.62s/it]                                                     {'loss': 0.6461, 'learning_rate': 0.0007008477123264848, 'epoch': 0.39}
 39%|███▉      | 228/588 [1:57:59<3:03:42, 30.62s/it] 39%|███▉      | 229/588 [1:58:29<3:02:28, 30.50s/it]                                                     {'loss': 0.6528, 'learning_rate': 0.000698321001907167, 'epoch': 0.39}
 39%|███▉      | 229/588 [1:58:29<3:02:28, 30.50s/it] 39%|███▉      | 230/588 [1:58:59<3:01:41, 30.45s/it]                                                     {'loss': 0.663, 'learning_rate': 0.0006957882670345458, 'epoch': 0.39}
 39%|███▉      | 230/588 [1:58:59<3:01:41, 30.45s/it] 39%|███▉      | 231/588 [1:59:30<3:01:07, 30.44s/it]                                                     {'loss': 0.6723, 'learning_rate': 0.0006932495846462261, 'epoch': 0.39}
 39%|███▉      | 231/588 [1:59:30<3:01:07, 30.44s/it] 39%|███▉      | 232/588 [2:00:00<3:00:34, 30.43s/it]                                                     {'loss': 0.6223, 'learning_rate': 0.000690705031860483, 'epoch': 0.39}
 39%|███▉      | 232/588 [2:00:00<3:00:34, 30.43s/it] 40%|███▉      | 233/588 [2:00:31<3:00:03, 30.43s/it]                                                     {'loss': 0.6495, 'learning_rate': 0.0006881546859739178, 'epoch': 0.4}
 40%|███▉      | 233/588 [2:00:31<3:00:03, 30.43s/it] 40%|███▉      | 234/588 [2:01:01<2:59:31, 30.43s/it]                                                     {'loss': 0.6496, 'learning_rate': 0.0006855986244591103, 'epoch': 0.4}
 40%|███▉      | 234/588 [2:01:01<2:59:31, 30.43s/it] 40%|███▉      | 235/588 [2:01:31<2:58:46, 30.39s/it]                                                     {'loss': 0.6286, 'learning_rate': 0.0006830369249622662, 'epoch': 0.4}
 40%|███▉      | 235/588 [2:01:31<2:58:46, 30.39s/it] 40%|████      | 236/588 [2:02:02<2:58:27, 30.42s/it]                                                     {'loss': 0.6362, 'learning_rate': 0.0006804696653008574, 'epoch': 0.4}
 40%|████      | 236/588 [2:02:02<2:58:27, 30.42s/it] 40%|████      | 237/588 [2:02:34<2:59:58, 30.76s/it]                                                     {'loss': 0.6592, 'learning_rate': 0.0006778969234612583, 'epoch': 0.4}
 40%|████      | 237/588 [2:02:34<2:59:58, 30.76s/it] 40%|████      | 238/588 [2:03:04<2:58:33, 30.61s/it]                                                     {'loss': 0.6634, 'learning_rate': 0.0006753187775963773, 'epoch': 0.4}
 40%|████      | 238/588 [2:03:04<2:58:33, 30.61s/it] 41%|████      | 239/588 [2:03:37<3:02:36, 31.39s/it]                                                     {'loss': 0.6126, 'learning_rate': 0.0006727353060232822, 'epoch': 0.41}
 41%|████      | 239/588 [2:03:37<3:02:36, 31.39s/it] 41%|████      | 240/588 [2:04:08<3:00:34, 31.13s/it]                                                     {'loss': 0.64, 'learning_rate': 0.0006701465872208216, 'epoch': 0.41}
 41%|████      | 240/588 [2:04:08<3:00:34, 31.13s/it] 41%|████      | 241/588 [2:04:38<2:58:46, 30.91s/it]                                                     {'loss': 0.6183, 'learning_rate': 0.0006675526998272404, 'epoch': 0.41}
 41%|████      | 241/588 [2:04:38<2:58:46, 30.91s/it] 41%|████      | 242/588 [2:05:08<2:57:29, 30.78s/it]                                                     {'loss': 0.6699, 'learning_rate': 0.0006649537226377914, 'epoch': 0.41}
 41%|████      | 242/588 [2:05:08<2:57:29, 30.78s/it] 41%|████▏     | 243/588 [2:05:39<2:56:15, 30.65s/it]                                                     {'loss': 0.6519, 'learning_rate': 0.0006623497346023419, 'epoch': 0.41}
 41%|████▏     | 243/588 [2:05:39<2:56:15, 30.65s/it] 41%|████▏     | 244/588 [2:06:09<2:55:33, 30.62s/it]                                                     {'loss': 0.6508, 'learning_rate': 0.0006597408148229741, 'epoch': 0.41}
 41%|████▏     | 244/588 [2:06:09<2:55:33, 30.62s/it] 42%|████▏     | 245/588 [2:06:49<3:10:16, 33.29s/it]                                                     {'loss': 0.6254, 'learning_rate': 0.0006571270425515843, 'epoch': 0.42}
 42%|████▏     | 245/588 [2:06:49<3:10:16, 33.29s/it] 42%|████▏     | 246/588 [2:07:19<3:04:47, 32.42s/it]                                                     {'loss': 0.6651, 'learning_rate': 0.0006545084971874737, 'epoch': 0.42}
 42%|████▏     | 246/588 [2:07:19<3:04:47, 32.42s/it] 42%|████▏     | 247/588 [2:07:50<3:01:00, 31.85s/it]                                                     {'loss': 0.6422, 'learning_rate': 0.0006518852582749373, 'epoch': 0.42}
 42%|████▏     | 247/588 [2:07:50<3:01:00, 31.85s/it] 42%|████▏     | 248/588 [2:08:20<2:58:32, 31.51s/it]                                                     {'loss': 0.6312, 'learning_rate': 0.0006492574055008473, 'epoch': 0.42}
 42%|████▏     | 248/588 [2:08:20<2:58:32, 31.51s/it] 42%|████▏     | 249/588 [2:08:51<2:56:05, 31.17s/it]                                                     {'loss': 0.6147, 'learning_rate': 0.0006466250186922324, 'epoch': 0.42}
 42%|████▏     | 249/588 [2:08:51<2:56:05, 31.17s/it] 43%|████▎     | 250/588 [2:09:21<2:54:11, 30.92s/it]                                                     {'loss': 0.6129, 'learning_rate': 0.0006439881778138531, 'epoch': 0.43}
 43%|████▎     | 250/588 [2:09:21<2:54:11, 30.92s/it] 43%|████▎     | 251/588 [2:09:52<2:52:52, 30.78s/it]                                                     {'loss': 0.618, 'learning_rate': 0.0006413469629657723, 'epoch': 0.43}
 43%|████▎     | 251/588 [2:09:52<2:52:52, 30.78s/it] 43%|████▎     | 252/588 [2:10:22<2:51:59, 30.71s/it]                                                     {'loss': 0.6668, 'learning_rate': 0.0006387014543809223, 'epoch': 0.43}
 43%|████▎     | 252/588 [2:10:22<2:51:59, 30.71s/it] 43%|████▎     | 253/588 [2:10:52<2:50:19, 30.51s/it]                                                     {'loss': 0.6492, 'learning_rate': 0.0006360517324226675, 'epoch': 0.43}
 43%|████▎     | 253/588 [2:10:52<2:50:19, 30.51s/it] 43%|████▎     | 254/588 [2:11:24<2:52:23, 30.97s/it]                                                     {'loss': 0.6269, 'learning_rate': 0.0006333978775823631, 'epoch': 0.43}
 43%|████▎     | 254/588 [2:11:24<2:52:23, 30.97s/it] 43%|████▎     | 255/588 [2:11:55<2:51:27, 30.89s/it]                                                     {'loss': 0.6363, 'learning_rate': 0.0006307399704769099, 'epoch': 0.43}
 43%|████▎     | 255/588 [2:11:55<2:51:27, 30.89s/it] 44%|████▎     | 256/588 [2:12:25<2:49:57, 30.72s/it]                                                     {'loss': 0.6691, 'learning_rate': 0.0006280780918463057, 'epoch': 0.44}
 44%|████▎     | 256/588 [2:12:25<2:49:57, 30.72s/it] 44%|████▎     | 257/588 [2:12:56<2:49:12, 30.67s/it]                                                     {'loss': 0.6553, 'learning_rate': 0.0006254123225511922, 'epoch': 0.44}
 44%|████▎     | 257/588 [2:12:56<2:49:12, 30.67s/it] 44%|████▍     | 258/588 [2:13:26<2:48:37, 30.66s/it]                                                     {'loss': 0.6457, 'learning_rate': 0.0006227427435703996, 'epoch': 0.44}
 44%|████▍     | 258/588 [2:13:26<2:48:37, 30.66s/it] 44%|████▍     | 259/588 [2:13:57<2:48:05, 30.65s/it]                                                     {'loss': 0.6452, 'learning_rate': 0.0006200694359984848, 'epoch': 0.44}
 44%|████▍     | 259/588 [2:13:57<2:48:05, 30.65s/it] 44%|████▍     | 260/588 [2:14:28<2:47:14, 30.59s/it]                                                     {'loss': 0.6456, 'learning_rate': 0.0006173924810432705, 'epoch': 0.44}
 44%|████▍     | 260/588 [2:14:28<2:47:14, 30.59s/it] 44%|████▍     | 261/588 [2:14:58<2:46:17, 30.51s/it]                                                     {'loss': 0.6698, 'learning_rate': 0.0006147119600233758, 'epoch': 0.44}
 44%|████▍     | 261/588 [2:14:58<2:46:17, 30.51s/it] 45%|████▍     | 262/588 [2:15:28<2:45:45, 30.51s/it]                                                     {'loss': 0.6229, 'learning_rate': 0.000612027954365748, 'epoch': 0.45}
 45%|████▍     | 262/588 [2:15:28<2:45:45, 30.51s/it] 45%|████▍     | 263/588 [2:15:59<2:45:06, 30.48s/it]                                                     {'loss': 0.6639, 'learning_rate': 0.0006093405456031879, 'epoch': 0.45}
 45%|████▍     | 263/588 [2:15:59<2:45:06, 30.48s/it] 45%|████▍     | 264/588 [2:16:29<2:44:26, 30.45s/it]                                                     {'loss': 0.6795, 'learning_rate': 0.0006066498153718734, 'epoch': 0.45}
 45%|████▍     | 264/588 [2:16:29<2:44:26, 30.45s/it] 45%|████▌     | 265/588 [2:17:00<2:43:51, 30.44s/it]                                                     {'loss': 0.6285, 'learning_rate': 0.0006039558454088796, 'epoch': 0.45}
 45%|████▌     | 265/588 [2:17:00<2:43:51, 30.44s/it] 45%|████▌     | 266/588 [2:17:30<2:43:37, 30.49s/it]                                                     {'loss': 0.5934, 'learning_rate': 0.0006012587175496961, 'epoch': 0.45}
 45%|████▌     | 266/588 [2:17:30<2:43:37, 30.49s/it] 45%|████▌     | 267/588 [2:18:01<2:43:12, 30.51s/it]                                                     {'loss': 0.6319, 'learning_rate': 0.0005985585137257401, 'epoch': 0.45}
 45%|████▌     | 267/588 [2:18:01<2:43:12, 30.51s/it] 46%|████▌     | 268/588 [2:18:31<2:42:48, 30.53s/it]                                                     {'loss': 0.5973, 'learning_rate': 0.0005958553159618693, 'epoch': 0.46}
 46%|████▌     | 268/588 [2:18:31<2:42:48, 30.53s/it] 46%|████▌     | 269/588 [2:19:02<2:42:11, 30.51s/it]                                                     {'loss': 0.645, 'learning_rate': 0.0005931492063738881, 'epoch': 0.46}
 46%|████▌     | 269/588 [2:19:02<2:42:11, 30.51s/it] 46%|████▌     | 270/588 [2:19:42<2:56:55, 33.38s/it]                                                     {'loss': 0.6002, 'learning_rate': 0.000590440267166055, 'epoch': 0.46}
 46%|████▌     | 270/588 [2:19:42<2:56:55, 33.38s/it] 46%|████▌     | 271/588 [2:20:21<3:05:35, 35.13s/it]                                                     {'loss': 0.6107, 'learning_rate': 0.0005877285806285841, 'epoch': 0.46}
 46%|████▌     | 271/588 [2:20:21<3:05:35, 35.13s/it] 46%|████▋     | 272/588 [2:20:57<3:07:05, 35.52s/it]                                                     {'loss': 0.6308, 'learning_rate': 0.0005850142291351465, 'epoch': 0.46}
 46%|████▋     | 272/588 [2:20:57<3:07:05, 35.52s/it] 46%|████▋     | 273/588 [2:21:30<3:02:27, 34.75s/it]                                                     {'loss': 0.6276, 'learning_rate': 0.000582297295140367, 'epoch': 0.46}
 46%|████▋     | 273/588 [2:21:30<3:02:27, 34.75s/it] 47%|████▋     | 274/588 [2:22:01<2:55:59, 33.63s/it]                                                     {'loss': 0.652, 'learning_rate': 0.0005795778611773197, 'epoch': 0.47}
 47%|████▋     | 274/588 [2:22:01<2:55:59, 33.63s/it] 47%|████▋     | 275/588 [2:22:38<3:00:26, 34.59s/it]                                                     {'loss': 0.5959, 'learning_rate': 0.0005768560098550213, 'epoch': 0.47}
 47%|████▋     | 275/588 [2:22:38<3:00:26, 34.59s/it] 47%|████▋     | 276/588 [2:23:09<2:53:21, 33.34s/it]                                                     {'loss': 0.6409, 'learning_rate': 0.000574131823855921, 'epoch': 0.47}
 47%|████▋     | 276/588 [2:23:09<2:53:21, 33.34s/it] 47%|████▋     | 277/588 [2:23:39<2:48:30, 32.51s/it]                                                     {'loss': 0.6215, 'learning_rate': 0.0005714053859333893, 'epoch': 0.47}
 47%|████▋     | 277/588 [2:23:39<2:48:30, 32.51s/it] 47%|████▋     | 278/588 [2:24:17<2:55:49, 34.03s/it]                                                     {'loss': 0.6586, 'learning_rate': 0.0005686767789092041, 'epoch': 0.47}
 47%|████▋     | 278/588 [2:24:17<2:55:49, 34.03s/it] 47%|████▋     | 279/588 [2:24:51<2:55:05, 34.00s/it]                                                     {'loss': 0.6361, 'learning_rate': 0.0005659460856710345, 'epoch': 0.47}
 47%|████▋     | 279/588 [2:24:51<2:55:05, 34.00s/it] 48%|████▊     | 280/588 [2:25:21<2:48:44, 32.87s/it]                                                     {'loss': 0.5996, 'learning_rate': 0.0005632133891699232, 'epoch': 0.48}
 48%|████▊     | 280/588 [2:25:21<2:48:44, 32.87s/it] 48%|████▊     | 281/588 [2:25:51<2:44:16, 32.11s/it]                                                     {'loss': 0.6148, 'learning_rate': 0.0005604787724177666, 'epoch': 0.48}
 48%|████▊     | 281/588 [2:25:51<2:44:16, 32.11s/it] 48%|████▊     | 282/588 [2:26:22<2:41:14, 31.62s/it]                                                     {'loss': 0.5814, 'learning_rate': 0.0005577423184847932, 'epoch': 0.48}
 48%|████▊     | 282/588 [2:26:22<2:41:14, 31.62s/it] 48%|████▊     | 283/588 [2:26:52<2:38:54, 31.26s/it]                                                     {'loss': 0.6597, 'learning_rate': 0.0005550041104970397, 'epoch': 0.48}
 48%|████▊     | 283/588 [2:26:52<2:38:54, 31.26s/it] 48%|████▊     | 284/588 [2:27:22<2:36:50, 30.96s/it]                                                     {'loss': 0.6757, 'learning_rate': 0.0005522642316338268, 'epoch': 0.48}
 48%|████▊     | 284/588 [2:27:22<2:36:50, 30.96s/it] 48%|████▊     | 285/588 [2:28:04<2:51:34, 33.98s/it]                                                     {'loss': 0.6339, 'learning_rate': 0.0005495227651252315, 'epoch': 0.48}
 48%|████▊     | 285/588 [2:28:04<2:51:34, 33.98s/it] 49%|████▊     | 286/588 [2:28:34<2:45:55, 32.97s/it]                                                     {'loss': 0.7028, 'learning_rate': 0.000546779794249559, 'epoch': 0.49}
 49%|████▊     | 286/588 [2:28:34<2:45:55, 32.97s/it] 49%|████▉     | 287/588 [2:29:04<2:41:16, 32.15s/it]                                                     {'loss': 0.6309, 'learning_rate': 0.0005440354023308134, 'epoch': 0.49}
 49%|████▉     | 287/588 [2:29:04<2:41:16, 32.15s/it] 49%|████▉     | 288/588 [2:29:35<2:38:15, 31.65s/it]                                                     {'loss': 0.6607, 'learning_rate': 0.0005412896727361663, 'epoch': 0.49}
 49%|████▉     | 288/588 [2:29:35<2:38:15, 31.65s/it] 49%|████▉     | 289/588 [2:30:18<2:54:56, 35.11s/it]                                                     {'loss': 0.6137, 'learning_rate': 0.0005385426888734237, 'epoch': 0.49}
 49%|████▉     | 289/588 [2:30:18<2:54:56, 35.11s/it] 49%|████▉     | 290/588 [2:30:48<2:46:57, 33.62s/it]                                                     {'loss': 0.6079, 'learning_rate': 0.0005357945341884936, 'epoch': 0.49}
 49%|████▉     | 290/588 [2:30:48<2:46:57, 33.62s/it] 49%|████▉     | 291/588 [2:31:19<2:41:38, 32.66s/it]                                                     {'loss': 0.6071, 'learning_rate': 0.0005330452921628497, 'epoch': 0.49}
 49%|████▉     | 291/588 [2:31:19<2:41:38, 32.66s/it] 50%|████▉     | 292/588 [2:31:49<2:38:00, 32.03s/it]                                                     {'loss': 0.6262, 'learning_rate': 0.0005302950463109969, 'epoch': 0.5}
 50%|████▉     | 292/588 [2:31:49<2:38:00, 32.03s/it] 50%|████▉     | 293/588 [2:32:35<2:58:28, 36.30s/it]                                                     {'loss': 0.6625, 'learning_rate': 0.0005275438801779327, 'epoch': 0.5}
 50%|████▉     | 293/588 [2:32:35<2:58:28, 36.30s/it] 50%|█████     | 294/588 [2:33:06<2:49:31, 34.60s/it]                                                     {'loss': 0.6425, 'learning_rate': 0.0005247918773366112, 'epoch': 0.5}
 50%|█████     | 294/588 [2:33:06<2:49:31, 34.60s/it] 50%|█████     | 295/588 [2:33:36<2:42:44, 33.33s/it]                                                     {'loss': 0.6226, 'learning_rate': 0.0005220391213854028, 'epoch': 0.5}
 50%|█████     | 295/588 [2:33:36<2:42:44, 33.33s/it] 50%|█████     | 296/588 [2:34:27<3:07:03, 38.44s/it]                                                     {'loss': 0.583, 'learning_rate': 0.0005192856959455552, 'epoch': 0.5}
 50%|█████     | 296/588 [2:34:27<3:07:03, 38.44s/it] 51%|█████     | 297/588 [2:34:57<2:55:01, 36.09s/it]                                                     {'loss': 0.6429, 'learning_rate': 0.0005165316846586541, 'epoch': 0.51}
 51%|█████     | 297/588 [2:34:57<2:55:01, 36.09s/it] 51%|█████     | 298/588 [2:35:27<2:44:53, 34.12s/it]                                                     {'loss': 0.6775, 'learning_rate': 0.0005137771711840811, 'epoch': 0.51}
 51%|█████     | 298/588 [2:35:27<2:44:53, 34.12s/it] 51%|█████     | 299/588 [2:35:58<2:39:18, 33.08s/it]                                                     {'loss': 0.588, 'learning_rate': 0.0005110222391964728, 'epoch': 0.51}
 51%|█████     | 299/588 [2:35:58<2:39:18, 33.08s/it] 51%|█████     | 300/588 [2:36:28<2:34:55, 32.28s/it]                                                     {'loss': 0.6283, 'learning_rate': 0.0005082669723831793, 'epoch': 0.51}
 51%|█████     | 300/588 [2:36:28<2:34:55, 32.28s/it] 51%|█████     | 301/588 [2:36:58<2:31:43, 31.72s/it]                                                     {'loss': 0.6442, 'learning_rate': 0.0005055114544417218, 'epoch': 0.51}
 51%|█████     | 301/588 [2:36:58<2:31:43, 31.72s/it] 51%|█████▏    | 302/588 [2:37:29<2:29:23, 31.34s/it]                                                     {'loss': 0.6064, 'learning_rate': 0.0005027557690772503, 'epoch': 0.51}
 51%|█████▏    | 302/588 [2:37:29<2:29:23, 31.34s/it] 52%|█████▏    | 303/588 [2:37:59<2:27:45, 31.11s/it]                                                     {'loss': 0.6203, 'learning_rate': 0.0005, 'epoch': 0.52}
 52%|█████▏    | 303/588 [2:37:59<2:27:45, 31.11s/it] 52%|█████▏    | 304/588 [2:38:30<2:26:22, 30.92s/it]                                                     {'loss': 0.6328, 'learning_rate': 0.0004972442309227498, 'epoch': 0.52}
 52%|█████▏    | 304/588 [2:38:30<2:26:22, 30.92s/it] 52%|█████▏    | 305/588 [2:39:00<2:25:15, 30.80s/it]                                                     {'loss': 0.6274, 'learning_rate': 0.0004944885455582783, 'epoch': 0.52}
 52%|█████▏    | 305/588 [2:39:00<2:25:15, 30.80s/it] 52%|█████▏    | 306/588 [2:39:59<3:04:35, 39.27s/it]                                                     {'loss': 0.6036, 'learning_rate': 0.0004917330276168208, 'epoch': 0.52}
 52%|█████▏    | 306/588 [2:39:59<3:04:35, 39.27s/it] 52%|█████▏    | 307/588 [2:40:30<2:51:46, 36.68s/it]                                                     {'loss': 0.6502, 'learning_rate': 0.0004889777608035273, 'epoch': 0.52}
 52%|█████▏    | 307/588 [2:40:30<2:51:46, 36.68s/it] 52%|█████▏    | 308/588 [2:41:01<2:42:32, 34.83s/it]                                                     {'loss': 0.6009, 'learning_rate': 0.0004862228288159191, 'epoch': 0.52}
 52%|█████▏    | 308/588 [2:41:01<2:42:32, 34.83s/it] 53%|█████▎    | 309/588 [2:41:38<2:45:05, 35.50s/it]                                                     {'loss': 0.639, 'learning_rate': 0.0004834683153413459, 'epoch': 0.53}
 53%|█████▎    | 309/588 [2:41:38<2:45:05, 35.50s/it] 53%|█████▎    | 310/588 [2:42:08<2:37:37, 34.02s/it]                                                     {'loss': 0.6301, 'learning_rate': 0.00048071430405444474, 'epoch': 0.53}
 53%|█████▎    | 310/588 [2:42:08<2:37:37, 34.02s/it] 53%|█████▎    | 311/588 [2:42:45<2:41:28, 34.98s/it]                                                     {'loss': 0.6109, 'learning_rate': 0.00047796087861459735, 'epoch': 0.53}
 53%|█████▎    | 311/588 [2:42:45<2:41:28, 34.98s/it] 53%|█████▎    | 312/588 [2:43:23<2:44:31, 35.77s/it]                                                     {'loss': 0.6152, 'learning_rate': 0.0004752081226633888, 'epoch': 0.53}
 53%|█████▎    | 312/588 [2:43:23<2:44:31, 35.77s/it] 53%|█████▎    | 313/588 [2:43:53<2:36:25, 34.13s/it]                                                     {'loss': 0.6134, 'learning_rate': 0.0004724561198220672, 'epoch': 0.53}
 53%|█████▎    | 313/588 [2:43:53<2:36:25, 34.13s/it] 53%|█████▎    | 314/588 [2:44:24<2:30:41, 33.00s/it]                                                     {'loss': 0.6778, 'learning_rate': 0.0004697049536890033, 'epoch': 0.53}
 53%|█████▎    | 314/588 [2:44:24<2:30:41, 33.00s/it] 54%|█████▎    | 315/588 [2:44:54<2:26:32, 32.21s/it]                                                     {'loss': 0.6201, 'learning_rate': 0.00046695470783715033, 'epoch': 0.54}
 54%|█████▎    | 315/588 [2:44:54<2:26:32, 32.21s/it] 54%|█████▎    | 316/588 [2:45:25<2:23:48, 31.72s/it]                                                     {'loss': 0.6102, 'learning_rate': 0.00046420546581150664, 'epoch': 0.54}
 54%|█████▎    | 316/588 [2:45:25<2:23:48, 31.72s/it] 54%|█████▍    | 317/588 [2:45:58<2:26:10, 32.36s/it]                                                     {'loss': 0.6313, 'learning_rate': 0.0004614573111265764, 'epoch': 0.54}
 54%|█████▍    | 317/588 [2:45:58<2:26:10, 32.36s/it] 54%|█████▍    | 318/588 [2:46:30<2:24:09, 32.03s/it]                                                     {'loss': 0.6205, 'learning_rate': 0.0004587103272638339, 'epoch': 0.54}
 54%|█████▍    | 318/588 [2:46:30<2:24:09, 32.03s/it] 54%|█████▍    | 319/588 [2:47:00<2:21:29, 31.56s/it]                                                     {'loss': 0.5897, 'learning_rate': 0.00045596459766918677, 'epoch': 0.54}
 54%|█████▍    | 319/588 [2:47:00<2:21:29, 31.56s/it] 54%|█████▍    | 320/588 [2:47:31<2:19:28, 31.22s/it]                                                     {'loss': 0.6226, 'learning_rate': 0.00045322020575044115, 'epoch': 0.54}
 54%|█████▍    | 320/588 [2:47:31<2:19:28, 31.22s/it] 55%|█████▍    | 321/588 [2:48:04<2:22:23, 32.00s/it]                                                     {'loss': 0.6457, 'learning_rate': 0.0004504772348747687, 'epoch': 0.55}
 55%|█████▍    | 321/588 [2:48:04<2:22:23, 32.00s/it] 55%|█████▍    | 322/588 [2:48:35<2:19:44, 31.52s/it]                                                     {'loss': 0.6047, 'learning_rate': 0.00044773576836617336, 'epoch': 0.55}
 55%|█████▍    | 322/588 [2:48:35<2:19:44, 31.52s/it] 55%|█████▍    | 323/588 [2:49:18<2:33:58, 34.86s/it]                                                     {'loss': 0.6563, 'learning_rate': 0.0004449958895029604, 'epoch': 0.55}
 55%|█████▍    | 323/588 [2:49:18<2:33:58, 34.86s/it] 55%|█████▌    | 324/588 [2:49:48<2:27:32, 33.53s/it]                                                     {'loss': 0.6016, 'learning_rate': 0.00044225768151520694, 'epoch': 0.55}
 55%|█████▌    | 324/588 [2:49:48<2:27:32, 33.53s/it] 55%|█████▌    | 325/588 [2:50:59<3:16:35, 44.85s/it]                                                     {'loss': 0.6132, 'learning_rate': 0.0004395212275822335, 'epoch': 0.55}
 55%|█████▌    | 325/588 [2:50:59<3:16:35, 44.85s/it] 55%|█████▌    | 326/588 [2:51:30<2:56:49, 40.50s/it]                                                     {'loss': 0.5888, 'learning_rate': 0.0004367866108300769, 'epoch': 0.55}
 55%|█████▌    | 326/588 [2:51:30<2:56:49, 40.50s/it] 56%|█████▌    | 327/588 [2:52:00<2:43:06, 37.49s/it]                                                     {'loss': 0.6337, 'learning_rate': 0.0004340539143289655, 'epoch': 0.56}
 56%|█████▌    | 327/588 [2:52:00<2:43:06, 37.49s/it] 56%|█████▌    | 328/588 [2:52:30<2:33:10, 35.35s/it]                                                     {'loss': 0.6198, 'learning_rate': 0.0004313232210907959, 'epoch': 0.56}
 56%|█████▌    | 328/588 [2:52:30<2:33:10, 35.35s/it] 56%|█████▌    | 329/588 [2:53:01<2:26:02, 33.83s/it]                                                     {'loss': 0.6505, 'learning_rate': 0.0004285946140666107, 'epoch': 0.56}
 56%|█████▌    | 329/588 [2:53:01<2:26:02, 33.83s/it] 56%|█████▌    | 330/588 [2:53:31<2:21:13, 32.84s/it]                                                     {'loss': 0.6219, 'learning_rate': 0.00042586817614407896, 'epoch': 0.56}
 56%|█████▌    | 330/588 [2:53:31<2:21:13, 32.84s/it] 56%|█████▋    | 331/588 [2:54:02<2:17:27, 32.09s/it]                                                     {'loss': 0.6369, 'learning_rate': 0.0004231439901449787, 'epoch': 0.56}
 56%|█████▋    | 331/588 [2:54:02<2:17:27, 32.09s/it] 56%|█████▋    | 332/588 [2:54:31<2:14:04, 31.43s/it]                                                     {'loss': 0.5905, 'learning_rate': 0.0004204221388226803, 'epoch': 0.56}
 56%|█████▋    | 332/588 [2:54:31<2:14:04, 31.43s/it] 57%|█████▋    | 333/588 [2:55:02<2:11:57, 31.05s/it]                                                     {'loss': 0.6134, 'learning_rate': 0.00041770270485963295, 'epoch': 0.57}
 57%|█████▋    | 333/588 [2:55:02<2:11:57, 31.05s/it] 57%|█████▋    | 334/588 [2:55:32<2:10:35, 30.85s/it]                                                     {'loss': 0.5955, 'learning_rate': 0.00041498577086485354, 'epoch': 0.57}
 57%|█████▋    | 334/588 [2:55:32<2:10:35, 30.85s/it] 57%|█████▋    | 335/588 [2:56:02<2:09:24, 30.69s/it]                                                     {'loss': 0.6588, 'learning_rate': 0.000412271419371416, 'epoch': 0.57}
 57%|█████▋    | 335/588 [2:56:02<2:09:24, 30.69s/it] 57%|█████▋    | 336/588 [2:56:33<2:08:35, 30.62s/it]                                                     {'loss': 0.6262, 'learning_rate': 0.0004095597328339452, 'epoch': 0.57}
 57%|█████▋    | 336/588 [2:56:33<2:08:35, 30.62s/it] 57%|█████▋    | 337/588 [2:57:03<2:07:47, 30.55s/it]                                                     {'loss': 0.6186, 'learning_rate': 0.000406850793626112, 'epoch': 0.57}
 57%|█████▋    | 337/588 [2:57:03<2:07:47, 30.55s/it] 57%|█████▋    | 338/588 [2:57:33<2:06:57, 30.47s/it]                                                     {'loss': 0.6401, 'learning_rate': 0.00040414468403813093, 'epoch': 0.57}
 57%|█████▋    | 338/588 [2:57:33<2:06:57, 30.47s/it] 58%|█████▊    | 339/588 [2:58:04<2:06:22, 30.45s/it]                                                     {'loss': 0.6188, 'learning_rate': 0.0004014414862742599, 'epoch': 0.58}
 58%|█████▊    | 339/588 [2:58:04<2:06:22, 30.45s/it] 58%|█████▊    | 340/588 [2:58:34<2:05:38, 30.40s/it]                                                     {'loss': 0.6257, 'learning_rate': 0.00039874128245030407, 'epoch': 0.58}
 58%|█████▊    | 340/588 [2:58:34<2:05:38, 30.40s/it] 58%|█████▊    | 341/588 [2:59:05<2:05:12, 30.42s/it]                                                     {'loss': 0.6282, 'learning_rate': 0.0003960441545911204, 'epoch': 0.58}
 58%|█████▊    | 341/588 [2:59:05<2:05:12, 30.42s/it] 58%|█████▊    | 342/588 [2:59:35<2:04:38, 30.40s/it]                                                     {'loss': 0.6029, 'learning_rate': 0.00039335018462812664, 'epoch': 0.58}
 58%|█████▊    | 342/588 [2:59:35<2:04:38, 30.40s/it] 58%|█████▊    | 343/588 [3:00:05<2:03:54, 30.35s/it]                                                     {'loss': 0.6183, 'learning_rate': 0.00039065945439681213, 'epoch': 0.58}
 58%|█████▊    | 343/588 [3:00:05<2:03:54, 30.35s/it] 59%|█████▊    | 344/588 [3:00:35<2:03:18, 30.32s/it]                                                     {'loss': 0.6633, 'learning_rate': 0.00038797204563425207, 'epoch': 0.59}
 59%|█████▊    | 344/588 [3:00:35<2:03:18, 30.32s/it] 59%|█████▊    | 345/588 [3:01:06<2:03:02, 30.38s/it]                                                     {'loss': 0.6148, 'learning_rate': 0.00038528803997662424, 'epoch': 0.59}
 59%|█████▊    | 345/588 [3:01:06<2:03:02, 30.38s/it] 59%|█████▉    | 346/588 [3:01:36<2:02:35, 30.40s/it]                                                     {'loss': 0.6018, 'learning_rate': 0.00038260751895672956, 'epoch': 0.59}
 59%|█████▉    | 346/588 [3:01:36<2:02:35, 30.40s/it] 59%|█████▉    | 347/588 [3:02:07<2:01:57, 30.36s/it]                                                     {'loss': 0.6133, 'learning_rate': 0.00037993056400151514, 'epoch': 0.59}
 59%|█████▉    | 347/588 [3:02:07<2:01:57, 30.36s/it] 59%|█████▉    | 348/588 [3:02:37<2:01:26, 30.36s/it]                                                     {'loss': 0.6201, 'learning_rate': 0.00037725725642960046, 'epoch': 0.59}
 59%|█████▉    | 348/588 [3:02:37<2:01:26, 30.36s/it] 59%|█████▉    | 349/588 [3:03:07<2:01:05, 30.40s/it]                                                     {'loss': 0.6592, 'learning_rate': 0.0003745876774488076, 'epoch': 0.59}
 59%|█████▉    | 349/588 [3:03:07<2:01:05, 30.40s/it] 60%|█████▉    | 350/588 [3:03:38<2:00:37, 30.41s/it]                                                     {'loss': 0.632, 'learning_rate': 0.0003719219081536942, 'epoch': 0.6}
 60%|█████▉    | 350/588 [3:03:38<2:00:37, 30.41s/it] 60%|█████▉    | 351/588 [3:04:08<2:00:04, 30.40s/it]                                                     {'loss': 0.607, 'learning_rate': 0.00036926002952309013, 'epoch': 0.6}
 60%|█████▉    | 351/588 [3:04:08<2:00:04, 30.40s/it] 60%|█████▉    | 352/588 [3:04:39<1:59:28, 30.38s/it]                                                     {'loss': 0.5887, 'learning_rate': 0.0003666021224176369, 'epoch': 0.6}
 60%|█████▉    | 352/588 [3:04:39<1:59:28, 30.38s/it] 60%|██████    | 353/588 [3:05:09<1:59:13, 30.44s/it]                                                     {'loss': 0.5758, 'learning_rate': 0.0003639482675773324, 'epoch': 0.6}
 60%|██████    | 353/588 [3:05:09<1:59:13, 30.44s/it] 60%|██████    | 354/588 [3:05:40<1:59:14, 30.58s/it]                                                     {'loss': 0.6466, 'learning_rate': 0.00036129854561907783, 'epoch': 0.6}
 60%|██████    | 354/588 [3:05:40<1:59:14, 30.58s/it] 60%|██████    | 355/588 [3:06:10<1:58:27, 30.51s/it]                                                     {'loss': 0.6723, 'learning_rate': 0.0003586530370342279, 'epoch': 0.6}
 60%|██████    | 355/588 [3:06:10<1:58:27, 30.51s/it] 61%|██████    | 356/588 [3:06:41<1:57:43, 30.45s/it]                                                     {'loss': 0.6059, 'learning_rate': 0.000356011822186147, 'epoch': 0.61}
 61%|██████    | 356/588 [3:06:41<1:57:43, 30.45s/it] 61%|██████    | 357/588 [3:07:11<1:57:20, 30.48s/it]                                                     {'loss': 0.6094, 'learning_rate': 0.0003533749813077677, 'epoch': 0.61}
 61%|██████    | 357/588 [3:07:11<1:57:20, 30.48s/it] 61%|██████    | 358/588 [3:07:42<1:56:48, 30.47s/it]                                                     {'loss': 0.6406, 'learning_rate': 0.00035074259449915284, 'epoch': 0.61}
 61%|██████    | 358/588 [3:07:42<1:56:48, 30.47s/it] 61%|██████    | 359/588 [3:08:12<1:56:14, 30.46s/it]                                                     {'loss': 0.6384, 'learning_rate': 0.0003481147417250627, 'epoch': 0.61}
 61%|██████    | 359/588 [3:08:12<1:56:14, 30.46s/it] 61%|██████    | 360/588 [3:08:42<1:55:34, 30.41s/it]                                                     {'loss': 0.6173, 'learning_rate': 0.00034549150281252633, 'epoch': 0.61}
 61%|██████    | 360/588 [3:08:42<1:55:34, 30.41s/it] 61%|██████▏   | 361/588 [3:09:13<1:55:00, 30.40s/it]                                                     {'loss': 0.6471, 'learning_rate': 0.00034287295744841583, 'epoch': 0.61}
 61%|██████▏   | 361/588 [3:09:13<1:55:00, 30.40s/it] 62%|██████▏   | 362/588 [3:09:43<1:54:28, 30.39s/it]                                                     {'loss': 0.6107, 'learning_rate': 0.000340259185177026, 'epoch': 0.62}
 62%|██████▏   | 362/588 [3:09:43<1:54:28, 30.39s/it] 62%|██████▏   | 363/588 [3:10:14<1:53:57, 30.39s/it]                                                     {'loss': 0.6319, 'learning_rate': 0.0003376502653976583, 'epoch': 0.62}
 62%|██████▏   | 363/588 [3:10:14<1:53:57, 30.39s/it] 62%|██████▏   | 364/588 [3:10:44<1:53:23, 30.37s/it]                                                     {'loss': 0.6261, 'learning_rate': 0.0003350462773622086, 'epoch': 0.62}
 62%|██████▏   | 364/588 [3:10:44<1:53:23, 30.37s/it] 62%|██████▏   | 365/588 [3:11:14<1:53:04, 30.42s/it]                                                     {'loss': 0.6069, 'learning_rate': 0.0003324473001727597, 'epoch': 0.62}
 62%|██████▏   | 365/588 [3:11:14<1:53:04, 30.42s/it] 62%|██████▏   | 366/588 [3:11:45<1:52:29, 30.40s/it]                                                     {'loss': 0.6359, 'learning_rate': 0.0003298534127791785, 'epoch': 0.62}
 62%|██████▏   | 366/588 [3:11:45<1:52:29, 30.40s/it] 62%|██████▏   | 367/588 [3:12:14<1:50:56, 30.12s/it]                                                     {'loss': 0.638, 'learning_rate': 0.0003272646939767179, 'epoch': 0.62}
 62%|██████▏   | 367/588 [3:12:14<1:50:56, 30.12s/it] 63%|██████▎   | 368/588 [3:12:45<1:50:48, 30.22s/it]                                                     {'loss': 0.613, 'learning_rate': 0.00032468122240362287, 'epoch': 0.63}
 63%|██████▎   | 368/588 [3:12:45<1:50:48, 30.22s/it] 63%|██████▎   | 369/588 [3:13:15<1:50:30, 30.28s/it]                                                     {'loss': 0.6071, 'learning_rate': 0.0003221030765387417, 'epoch': 0.63}
 63%|██████▎   | 369/588 [3:13:15<1:50:30, 30.28s/it] 63%|██████▎   | 370/588 [3:13:45<1:49:59, 30.27s/it]                                                     {'loss': 0.5905, 'learning_rate': 0.00031953033469914275, 'epoch': 0.63}
 63%|██████▎   | 370/588 [3:13:45<1:49:59, 30.27s/it] 63%|██████▎   | 371/588 [3:14:21<1:55:41, 31.99s/it]                                                     {'loss': 0.6053, 'learning_rate': 0.0003169630750377337, 'epoch': 0.63}
 63%|██████▎   | 371/588 [3:14:21<1:55:41, 31.99s/it] 63%|██████▎   | 372/588 [3:14:55<1:57:05, 32.53s/it]                                                     {'loss': 0.6245, 'learning_rate': 0.0003144013755408895, 'epoch': 0.63}
 63%|██████▎   | 372/588 [3:14:55<1:57:05, 32.53s/it] 63%|██████▎   | 373/588 [3:15:25<1:54:07, 31.85s/it]                                                     {'loss': 0.6458, 'learning_rate': 0.0003118453140260823, 'epoch': 0.63}
 63%|██████▎   | 373/588 [3:15:25<1:54:07, 31.85s/it] 64%|██████▎   | 374/588 [3:16:04<2:00:41, 33.84s/it]                                                     {'loss': 0.6039, 'learning_rate': 0.00030929496813951696, 'epoch': 0.64}
 64%|██████▎   | 374/588 [3:16:04<2:00:41, 33.84s/it] 64%|██████▍   | 375/588 [3:16:34<1:56:33, 32.83s/it]                                                     {'loss': 0.6176, 'learning_rate': 0.000306750415353774, 'epoch': 0.64}
 64%|██████▍   | 375/588 [3:16:34<1:56:33, 32.83s/it] 64%|██████▍   | 376/588 [3:17:05<1:53:27, 32.11s/it]                                                     {'loss': 0.6178, 'learning_rate': 0.0003042117329654544, 'epoch': 0.64}
 64%|██████▍   | 376/588 [3:17:05<1:53:27, 32.11s/it] 64%|██████▍   | 377/588 [3:17:35<1:50:58, 31.56s/it]                                                     {'loss': 0.6043, 'learning_rate': 0.00030167899809283304, 'epoch': 0.64}
 64%|██████▍   | 377/588 [3:17:35<1:50:58, 31.56s/it] 64%|██████▍   | 378/588 [3:18:05<1:49:00, 31.15s/it]                                                     {'loss': 0.6569, 'learning_rate': 0.0002991522876735154, 'epoch': 0.64}
 64%|██████▍   | 378/588 [3:18:05<1:49:00, 31.15s/it] 64%|██████▍   | 379/588 [3:18:36<1:47:35, 30.89s/it]                                                     {'loss': 0.6113, 'learning_rate': 0.0002966316784621, 'epoch': 0.64}
 64%|██████▍   | 379/588 [3:18:36<1:47:35, 30.89s/it] 65%|██████▍   | 380/588 [3:19:06<1:46:37, 30.76s/it]                                                     {'loss': 0.606, 'learning_rate': 0.0002941172470278476, 'epoch': 0.65}
 65%|██████▍   | 380/588 [3:19:06<1:46:37, 30.76s/it] 65%|██████▍   | 381/588 [3:19:37<1:45:49, 30.68s/it]                                                     {'loss': 0.6197, 'learning_rate': 0.0002916090697523549, 'epoch': 0.65}
 65%|██████▍   | 381/588 [3:19:37<1:45:49, 30.68s/it] 65%|██████▍   | 382/588 [3:20:07<1:45:01, 30.59s/it]                                                     {'loss': 0.6211, 'learning_rate': 0.000289107222827234, 'epoch': 0.65}
 65%|██████▍   | 382/588 [3:20:07<1:45:01, 30.59s/it] 65%|██████▌   | 383/588 [3:20:37<1:44:11, 30.49s/it]                                                     {'loss': 0.621, 'learning_rate': 0.0002866117822517982, 'epoch': 0.65}
 65%|██████▌   | 383/588 [3:20:37<1:44:11, 30.49s/it] 65%|██████▌   | 384/588 [3:21:07<1:43:25, 30.42s/it]                                                     {'loss': 0.6431, 'learning_rate': 0.0002841228238307536, 'epoch': 0.65}
 65%|██████▌   | 384/588 [3:21:07<1:43:25, 30.42s/it] 65%|██████▌   | 385/588 [3:21:38<1:42:55, 30.42s/it]                                                     {'loss': 0.6338, 'learning_rate': 0.00028164042317189574, 'epoch': 0.65}
 65%|██████▌   | 385/588 [3:21:38<1:42:55, 30.42s/it] 66%|██████▌   | 386/588 [3:22:08<1:42:17, 30.39s/it]                                                     {'loss': 0.6328, 'learning_rate': 0.000279164655683813, 'epoch': 0.66}
 66%|██████▌   | 386/588 [3:22:08<1:42:17, 30.39s/it] 66%|██████▌   | 387/588 [3:22:39<1:41:51, 30.40s/it]                                                     {'loss': 0.6123, 'learning_rate': 0.00027669559657359676, 'epoch': 0.66}
 66%|██████▌   | 387/588 [3:22:39<1:41:51, 30.40s/it] 66%|██████▌   | 388/588 [3:23:09<1:41:09, 30.35s/it]                                                     {'loss': 0.6422, 'learning_rate': 0.00027423332084455543, 'epoch': 0.66}
 66%|██████▌   | 388/588 [3:23:09<1:41:09, 30.35s/it] 66%|██████▌   | 389/588 [3:23:39<1:40:34, 30.32s/it]                                                     {'loss': 0.5828, 'learning_rate': 0.0002717779032939367, 'epoch': 0.66}
 66%|██████▌   | 389/588 [3:23:39<1:40:34, 30.32s/it] 66%|██████▋   | 390/588 [3:24:09<1:40:00, 30.30s/it]                                                     {'loss': 0.6291, 'learning_rate': 0.00026932941851065615, 'epoch': 0.66}
 66%|██████▋   | 390/588 [3:24:09<1:40:00, 30.30s/it] 66%|██████▋   | 391/588 [3:24:40<1:39:35, 30.33s/it]                                                     {'loss': 0.6147, 'learning_rate': 0.0002668879408730299, 'epoch': 0.66}
 66%|██████▋   | 391/588 [3:24:40<1:39:35, 30.33s/it] 67%|██████▋   | 392/588 [3:25:10<1:39:06, 30.34s/it]                                                     {'loss': 0.6403, 'learning_rate': 0.0002644535445465164, 'epoch': 0.67}
 67%|██████▋   | 392/588 [3:25:10<1:39:06, 30.34s/it] 67%|██████▋   | 393/588 [3:25:40<1:38:28, 30.30s/it]                                                     {'loss': 0.637, 'learning_rate': 0.0002620263034814632, 'epoch': 0.67}
 67%|██████▋   | 393/588 [3:25:40<1:38:28, 30.30s/it] 67%|██████▋   | 394/588 [3:26:11<1:37:55, 30.28s/it]                                                     {'loss': 0.6465, 'learning_rate': 0.00025960629141086013, 'epoch': 0.67}
 67%|██████▋   | 394/588 [3:26:11<1:37:55, 30.28s/it] 67%|██████▋   | 395/588 [3:26:41<1:37:23, 30.28s/it]                                                     {'loss': 0.5968, 'learning_rate': 0.0002571935818481005, 'epoch': 0.67}
 67%|██████▋   | 395/588 [3:26:41<1:37:23, 30.28s/it] 67%|██████▋   | 396/588 [3:27:11<1:36:53, 30.28s/it]                                                     {'loss': 0.6114, 'learning_rate': 0.0002547882480847461, 'epoch': 0.67}
 67%|██████▋   | 396/588 [3:27:11<1:36:53, 30.28s/it] 68%|██████▊   | 397/588 [3:27:42<1:36:32, 30.33s/it]                                                     {'loss': 0.617, 'learning_rate': 0.0002523903631883028, 'epoch': 0.68}
 68%|██████▊   | 397/588 [3:27:42<1:36:32, 30.33s/it] 68%|██████▊   | 398/588 [3:28:12<1:36:01, 30.32s/it]                                                     {'loss': 0.6405, 'learning_rate': 0.0002500000000000001, 'epoch': 0.68}
 68%|██████▊   | 398/588 [3:28:12<1:36:01, 30.32s/it] 68%|██████▊   | 399/588 [3:28:42<1:35:31, 30.32s/it]                                                     {'loss': 0.639, 'learning_rate': 0.0002476172311325783, 'epoch': 0.68}
 68%|██████▊   | 399/588 [3:28:42<1:35:31, 30.32s/it] 68%|██████▊   | 400/588 [3:29:13<1:35:03, 30.34s/it]                                                     {'loss': 0.6134, 'learning_rate': 0.00024524212896808264, 'epoch': 0.68}
 68%|██████▊   | 400/588 [3:29:13<1:35:03, 30.34s/it] 68%|██████▊   | 401/588 [3:29:43<1:34:27, 30.31s/it]                                                     {'loss': 0.5932, 'learning_rate': 0.00024287476565566525, 'epoch': 0.68}
 68%|██████▊   | 401/588 [3:29:43<1:34:27, 30.31s/it] 68%|██████▊   | 402/588 [3:30:13<1:34:02, 30.34s/it]                                                     {'loss': 0.6073, 'learning_rate': 0.00024051521310939256, 'epoch': 0.68}
 68%|██████▊   | 402/588 [3:30:13<1:34:02, 30.34s/it] 69%|██████▊   | 403/588 [3:30:44<1:33:34, 30.35s/it]                                                     {'loss': 0.6266, 'learning_rate': 0.00023816354300606107, 'epoch': 0.69}
 69%|██████▊   | 403/588 [3:30:44<1:33:34, 30.35s/it] 69%|██████▊   | 404/588 [3:31:14<1:32:55, 30.30s/it]                                                     {'loss': 0.5941, 'learning_rate': 0.00023581982678302061, 'epoch': 0.69}
 69%|██████▊   | 404/588 [3:31:14<1:32:55, 30.30s/it] 69%|██████▉   | 405/588 [3:31:44<1:32:26, 30.31s/it]                                                     {'loss': 0.6319, 'learning_rate': 0.00023348413563600325, 'epoch': 0.69}
 69%|██████▉   | 405/588 [3:31:44<1:32:26, 30.31s/it] 69%|██████▉   | 406/588 [3:32:14<1:31:59, 30.33s/it]                                                     {'loss': 0.5778, 'learning_rate': 0.00023115654051696094, 'epoch': 0.69}
 69%|██████▉   | 406/588 [3:32:14<1:31:59, 30.33s/it] 69%|██████▉   | 407/588 [3:32:45<1:31:27, 30.32s/it]                                                     {'loss': 0.6286, 'learning_rate': 0.0002288371121319109, 'epoch': 0.69}
 69%|██████▉   | 407/588 [3:32:45<1:31:27, 30.32s/it] 69%|██████▉   | 408/588 [3:33:15<1:31:05, 30.37s/it]                                                     {'loss': 0.5628, 'learning_rate': 0.00022652592093878665, 'epoch': 0.69}
 69%|██████▉   | 408/588 [3:33:15<1:31:05, 30.37s/it] 70%|██████▉   | 409/588 [3:33:46<1:30:34, 30.36s/it]                                                     {'loss': 0.6478, 'learning_rate': 0.0002242230371452982, 'epoch': 0.7}
 70%|██████▉   | 409/588 [3:33:46<1:30:34, 30.36s/it] 70%|██████▉   | 410/588 [3:34:16<1:30:06, 30.37s/it]                                                     {'loss': 0.6201, 'learning_rate': 0.0002219285307067997, 'epoch': 0.7}
 70%|██████▉   | 410/588 [3:34:16<1:30:06, 30.37s/it] 70%|██████▉   | 411/588 [3:34:46<1:29:23, 30.30s/it]                                                     {'loss': 0.6115, 'learning_rate': 0.0002196424713241637, 'epoch': 0.7}
 70%|██████▉   | 411/588 [3:34:46<1:29:23, 30.30s/it] 70%|███████   | 412/588 [3:35:16<1:28:55, 30.31s/it]                                                     {'loss': 0.6186, 'learning_rate': 0.00021736492844166405, 'epoch': 0.7}
 70%|███████   | 412/588 [3:35:16<1:28:55, 30.31s/it] 70%|███████   | 413/588 [3:35:47<1:28:21, 30.30s/it]                                                     {'loss': 0.6142, 'learning_rate': 0.0002150959712448669, 'epoch': 0.7}
 70%|███████   | 413/588 [3:35:47<1:28:21, 30.30s/it] 70%|███████   | 414/588 [3:36:17<1:27:57, 30.33s/it]                                                     {'loss': 0.6123, 'learning_rate': 0.0002128356686585282, 'epoch': 0.7}
 70%|███████   | 414/588 [3:36:17<1:27:57, 30.33s/it] 71%|███████   | 415/588 [3:36:47<1:27:22, 30.30s/it]                                                     {'loss': 0.6419, 'learning_rate': 0.0002105840893445005, 'epoch': 0.71}
 71%|███████   | 415/588 [3:36:47<1:27:22, 30.30s/it] 71%|███████   | 416/588 [3:37:18<1:26:50, 30.30s/it]                                                     {'loss': 0.6305, 'learning_rate': 0.00020834130169964694, 'epoch': 0.71}
 71%|███████   | 416/588 [3:37:18<1:26:50, 30.30s/it] 71%|███████   | 417/588 [3:37:48<1:26:17, 30.28s/it]                                                     {'loss': 0.5944, 'learning_rate': 0.00020610737385376348, 'epoch': 0.71}
 71%|███████   | 417/588 [3:37:48<1:26:17, 30.28s/it] 71%|███████   | 418/588 [3:38:18<1:25:53, 30.32s/it]                                                     {'loss': 0.6051, 'learning_rate': 0.00020388237366751006, 'epoch': 0.71}
 71%|███████   | 418/588 [3:38:18<1:25:53, 30.32s/it] 71%|███████▏  | 419/588 [3:38:49<1:25:25, 30.33s/it]                                                     {'loss': 0.588, 'learning_rate': 0.00020166636873034804, 'epoch': 0.71}
 71%|███████▏  | 419/588 [3:38:49<1:25:25, 30.33s/it] 71%|███████▏  | 420/588 [3:39:19<1:24:53, 30.32s/it]                                                     {'loss': 0.6119, 'learning_rate': 0.00019945942635848747, 'epoch': 0.71}
 71%|███████▏  | 420/588 [3:39:19<1:24:53, 30.32s/it] 72%|███████▏  | 421/588 [3:39:48<1:23:37, 30.05s/it]                                                     {'loss': 0.602, 'learning_rate': 0.00019726161359284283, 'epoch': 0.72}
 72%|███████▏  | 421/588 [3:39:48<1:23:37, 30.05s/it] 72%|███████▏  | 422/588 [3:40:19<1:23:28, 30.17s/it]                                                     {'loss': 0.6044, 'learning_rate': 0.00019507299719699552, 'epoch': 0.72}
 72%|███████▏  | 422/588 [3:40:19<1:23:28, 30.17s/it] 72%|███████▏  | 423/588 [3:40:49<1:23:03, 30.20s/it]                                                     {'loss': 0.5889, 'learning_rate': 0.00019289364365516608, 'epoch': 0.72}
 72%|███████▏  | 423/588 [3:40:49<1:23:03, 30.20s/it] 72%|███████▏  | 424/588 [3:41:19<1:22:33, 30.20s/it]                                                     {'loss': 0.6002, 'learning_rate': 0.00019072361917019538, 'epoch': 0.72}
 72%|███████▏  | 424/588 [3:41:19<1:22:33, 30.20s/it] 72%|███████▏  | 425/588 [3:41:50<1:22:12, 30.26s/it]                                                     {'loss': 0.5957, 'learning_rate': 0.00018856298966153212, 'epoch': 0.72}
 72%|███████▏  | 425/588 [3:41:50<1:22:12, 30.26s/it] 72%|███████▏  | 426/588 [3:42:20<1:21:44, 30.28s/it]                                                     {'loss': 0.6267, 'learning_rate': 0.00018641182076323148, 'epoch': 0.72}
 72%|███████▏  | 426/588 [3:42:20<1:21:44, 30.28s/it] 73%|███████▎  | 427/588 [3:42:50<1:21:22, 30.33s/it]                                                     {'loss': 0.6307, 'learning_rate': 0.00018427017782196125, 'epoch': 0.73}
 73%|███████▎  | 427/588 [3:42:50<1:21:22, 30.33s/it] 73%|███████▎  | 428/588 [3:43:40<1:36:15, 36.10s/it]                                                     {'loss': 0.6299, 'learning_rate': 0.0001821381258950161, 'epoch': 0.73}
 73%|███████▎  | 428/588 [3:43:40<1:36:15, 36.10s/it] 73%|███████▎  | 429/588 [3:44:10<1:31:07, 34.39s/it]                                                     {'loss': 0.6143, 'learning_rate': 0.00018001572974834167, 'epoch': 0.73}
 73%|███████▎  | 429/588 [3:44:10<1:31:07, 34.39s/it] 73%|███████▎  | 430/588 [3:44:41<1:27:29, 33.22s/it]                                                     {'loss': 0.595, 'learning_rate': 0.00017790305385456795, 'epoch': 0.73}
 73%|███████▎  | 430/588 [3:44:41<1:27:29, 33.22s/it] 73%|███████▎  | 431/588 [3:45:11<1:24:41, 32.37s/it]                                                     {'loss': 0.5739, 'learning_rate': 0.00017580016239104925, 'epoch': 0.73}
 73%|███████▎  | 431/588 [3:45:11<1:24:41, 32.37s/it] 73%|███████▎  | 432/588 [3:45:42<1:22:44, 31.82s/it]                                                     {'loss': 0.5846, 'learning_rate': 0.00017370711923791565, 'epoch': 0.73}
 73%|███████▎  | 432/588 [3:45:42<1:22:44, 31.82s/it] 74%|███████▎  | 433/588 [3:46:11<1:20:31, 31.17s/it]                                                     {'loss': 0.6061, 'learning_rate': 0.00017162398797613282, 'epoch': 0.74}
 74%|███████▎  | 433/588 [3:46:11<1:20:31, 31.17s/it] 74%|███████▍  | 434/588 [3:46:42<1:19:32, 30.99s/it]                                                     {'loss': 0.579, 'learning_rate': 0.00016955083188556946, 'epoch': 0.74}
 74%|███████▍  | 434/588 [3:46:42<1:19:32, 30.99s/it] 74%|███████▍  | 435/588 [3:47:13<1:18:38, 30.84s/it]                                                     {'loss': 0.5849, 'learning_rate': 0.00016748771394307582, 'epoch': 0.74}
 74%|███████▍  | 435/588 [3:47:13<1:18:38, 30.84s/it] 74%|███████▍  | 436/588 [3:47:43<1:17:51, 30.73s/it]                                                     {'loss': 0.6604, 'learning_rate': 0.00016543469682057105, 'epoch': 0.74}
 74%|███████▍  | 436/588 [3:47:43<1:17:51, 30.73s/it] 74%|███████▍  | 437/588 [3:48:13<1:17:08, 30.65s/it]                                                     {'loss': 0.6604, 'learning_rate': 0.0001633918428831377, 'epoch': 0.74}
 74%|███████▍  | 437/588 [3:48:13<1:17:08, 30.65s/it] 74%|███████▍  | 438/588 [3:48:44<1:16:25, 30.57s/it]                                                     {'loss': 0.6151, 'learning_rate': 0.00016135921418712956, 'epoch': 0.74}
 74%|███████▍  | 438/588 [3:48:44<1:16:25, 30.57s/it] 75%|███████▍  | 439/588 [3:49:14<1:15:45, 30.51s/it]                                                     {'loss': 0.6351, 'learning_rate': 0.0001593368724782846, 'epoch': 0.75}
 75%|███████▍  | 439/588 [3:49:14<1:15:45, 30.51s/it] 75%|███████▍  | 440/588 [3:49:45<1:15:10, 30.48s/it]                                                     {'loss': 0.5867, 'learning_rate': 0.00015732487918985015, 'epoch': 0.75}
 75%|███████▍  | 440/588 [3:49:45<1:15:10, 30.48s/it] 75%|███████▌  | 441/588 [3:50:15<1:14:34, 30.44s/it]                                                     {'loss': 0.631, 'learning_rate': 0.0001553232954407171, 'epoch': 0.75}
 75%|███████▌  | 441/588 [3:50:15<1:14:34, 30.44s/it] 75%|███████▌  | 442/588 [3:50:45<1:13:53, 30.37s/it]                                                     {'loss': 0.6266, 'learning_rate': 0.0001533321820335624, 'epoch': 0.75}
 75%|███████▌  | 442/588 [3:50:45<1:13:53, 30.37s/it] 75%|███████▌  | 443/588 [3:51:16<1:13:25, 30.38s/it]                                                     {'loss': 0.6523, 'learning_rate': 0.0001513515994530023, 'epoch': 0.75}
 75%|███████▌  | 443/588 [3:51:16<1:13:25, 30.38s/it] 76%|███████▌  | 444/588 [3:51:46<1:12:42, 30.29s/it]                                                     {'loss': 0.5808, 'learning_rate': 0.00014938160786375572, 'epoch': 0.76}
 76%|███████▌  | 444/588 [3:51:46<1:12:42, 30.29s/it] 76%|███████▌  | 445/588 [3:52:16<1:12:13, 30.30s/it]                                                     {'loss': 0.6, 'learning_rate': 0.00014742226710881556, 'epoch': 0.76}
 76%|███████▌  | 445/588 [3:52:16<1:12:13, 30.30s/it] 76%|███████▌  | 446/588 [3:52:46<1:11:45, 30.32s/it]                                                     {'loss': 0.6066, 'learning_rate': 0.00014547363670763136, 'epoch': 0.76}
 76%|███████▌  | 446/588 [3:52:46<1:11:45, 30.32s/it] 76%|███████▌  | 447/588 [3:53:17<1:11:10, 30.29s/it]                                                     {'loss': 0.6414, 'learning_rate': 0.0001435357758543015, 'epoch': 0.76}
 76%|███████▌  | 447/588 [3:53:17<1:11:10, 30.29s/it] 76%|███████▌  | 448/588 [3:53:47<1:10:43, 30.31s/it]                                                     {'loss': 0.6346, 'learning_rate': 0.00014160874341577446, 'epoch': 0.76}
 76%|███████▌  | 448/588 [3:53:47<1:10:43, 30.31s/it] 76%|███████▋  | 449/588 [3:54:17<1:10:13, 30.31s/it]                                                     {'loss': 0.6023, 'learning_rate': 0.0001396925979300608, 'epoch': 0.76}
 76%|███████▋  | 449/588 [3:54:17<1:10:13, 30.31s/it] 77%|███████▋  | 450/588 [3:54:48<1:09:53, 30.39s/it]                                                     {'loss': 0.6151, 'learning_rate': 0.00013778739760445552, 'epoch': 0.77}
 77%|███████▋  | 450/588 [3:54:48<1:09:53, 30.39s/it] 77%|███████▋  | 451/588 [3:55:25<1:14:06, 32.46s/it]                                                     {'loss': 0.5777, 'learning_rate': 0.000135893200313769, 'epoch': 0.77}
 77%|███████▋  | 451/588 [3:55:25<1:14:06, 32.46s/it] 77%|███████▋  | 452/588 [3:55:55<1:12:04, 31.80s/it]                                                     {'loss': 0.6025, 'learning_rate': 0.00013401006359856915, 'epoch': 0.77}
 77%|███████▋  | 452/588 [3:55:55<1:12:04, 31.80s/it] 77%|███████▋  | 453/588 [3:56:26<1:10:28, 31.32s/it]                                                     {'loss': 0.6386, 'learning_rate': 0.0001321380446634342, 'epoch': 0.77}
 77%|███████▋  | 453/588 [3:56:26<1:10:28, 31.32s/it] 77%|███████▋  | 454/588 [3:56:56<1:09:16, 31.02s/it]                                                     {'loss': 0.6036, 'learning_rate': 0.00013027720037521397, 'epoch': 0.77}
 77%|███████▋  | 454/588 [3:56:56<1:09:16, 31.02s/it] 77%|███████▋  | 455/588 [3:57:26<1:08:15, 30.79s/it]                                                     {'loss': 0.6338, 'learning_rate': 0.00012842758726130281, 'epoch': 0.77}
 77%|███████▋  | 455/588 [3:57:26<1:08:15, 30.79s/it] 78%|███████▊  | 456/588 [3:57:56<1:07:27, 30.66s/it]                                                     {'loss': 0.615, 'learning_rate': 0.0001265892615079232, 'epoch': 0.78}
 78%|███████▊  | 456/588 [3:57:56<1:07:27, 30.66s/it] 78%|███████▊  | 457/588 [3:58:27<1:06:33, 30.49s/it]                                                     {'loss': 0.6393, 'learning_rate': 0.00012476227895841713, 'epoch': 0.78}
 78%|███████▊  | 457/588 [3:58:27<1:06:33, 30.49s/it] 78%|███████▊  | 458/588 [3:58:57<1:05:59, 30.46s/it]                                                     {'loss': 0.6118, 'learning_rate': 0.00012294669511155192, 'epoch': 0.78}
 78%|███████▊  | 458/588 [3:58:57<1:05:59, 30.46s/it] 78%|███████▊  | 459/588 [3:59:27<1:05:19, 30.38s/it]                                                     {'loss': 0.592, 'learning_rate': 0.00012114256511983274, 'epoch': 0.78}
 78%|███████▊  | 459/588 [3:59:27<1:05:19, 30.38s/it] 78%|███████▊  | 460/588 [3:59:58<1:04:50, 30.40s/it]                                                     {'loss': 0.606, 'learning_rate': 0.00011934994378782771, 'epoch': 0.78}
 78%|███████▊  | 460/588 [3:59:58<1:04:50, 30.40s/it] 78%|███████▊  | 461/588 [4:00:28<1:04:21, 30.41s/it]                                                     {'loss': 0.6231, 'learning_rate': 0.00011756888557050355, 'epoch': 0.78}
 78%|███████▊  | 461/588 [4:00:28<1:04:21, 30.41s/it] 79%|███████▊  | 462/588 [4:00:58<1:03:45, 30.36s/it]                                                     {'loss': 0.6109, 'learning_rate': 0.0001157994445715706, 'epoch': 0.79}
 79%|███████▊  | 462/588 [4:00:58<1:03:45, 30.36s/it] 79%|███████▊  | 463/588 [4:01:29<1:03:16, 30.37s/it]                                                     {'loss': 0.6104, 'learning_rate': 0.00011404167454183955, 'epoch': 0.79}
 79%|███████▊  | 463/588 [4:01:29<1:03:16, 30.37s/it] 79%|███████▉  | 464/588 [4:01:59<1:02:41, 30.34s/it]                                                     {'loss': 0.5872, 'learning_rate': 0.00011229562887758927, 'epoch': 0.79}
 79%|███████▉  | 464/588 [4:01:59<1:02:41, 30.34s/it] 79%|███████▉  | 465/588 [4:02:29<1:02:09, 30.32s/it]                                                     {'loss': 0.5727, 'learning_rate': 0.00011056136061894385, 'epoch': 0.79}
 79%|███████▉  | 465/588 [4:02:29<1:02:09, 30.32s/it] 79%|███████▉  | 466/588 [4:03:00<1:01:41, 30.34s/it]                                                     {'loss': 0.6173, 'learning_rate': 0.00010883892244826171, 'epoch': 0.79}
 79%|███████▉  | 466/588 [4:03:00<1:01:41, 30.34s/it] 79%|███████▉  | 467/588 [4:03:30<1:01:09, 30.33s/it]                                                     {'loss': 0.6039, 'learning_rate': 0.00010712836668853581, 'epoch': 0.79}
 79%|███████▉  | 467/588 [4:03:30<1:01:09, 30.33s/it] 80%|███████▉  | 468/588 [4:04:02<1:01:31, 30.76s/it]                                                     {'loss': 0.6217, 'learning_rate': 0.00010542974530180327, 'epoch': 0.8}
 80%|███████▉  | 468/588 [4:04:02<1:01:31, 30.76s/it] 80%|███████▉  | 469/588 [4:04:32<1:00:44, 30.62s/it]                                                     {'loss': 0.6223, 'learning_rate': 0.00010374310988756747, 'epoch': 0.8}
 80%|███████▉  | 469/588 [4:04:32<1:00:44, 30.62s/it] 80%|███████▉  | 470/588 [4:05:02<1:00:06, 30.56s/it]                                                     {'loss': 0.6328, 'learning_rate': 0.00010206851168123076, 'epoch': 0.8}
 80%|███████▉  | 470/588 [4:05:02<1:00:06, 30.56s/it] 80%|████████  | 471/588 [4:05:33<59:33, 30.54s/it]                                                     {'loss': 0.5899, 'learning_rate': 0.00010040600155253765, 'epoch': 0.8}
 80%|████████  | 471/588 [4:05:33<59:33, 30.54s/it] 80%|████████  | 472/588 [4:06:03<58:51, 30.44s/it]                                                   {'loss': 0.6612, 'learning_rate': 9.875563000402948e-05, 'epoch': 0.8}
 80%|████████  | 472/588 [4:06:03<58:51, 30.44s/it] 80%|████████  | 473/588 [4:06:33<58:08, 30.33s/it]                                                   {'loss': 0.6415, 'learning_rate': 9.711744716951093e-05, 'epoch': 0.8}
 80%|████████  | 473/588 [4:06:33<58:08, 30.33s/it] 81%|████████  | 474/588 [4:07:04<57:42, 30.38s/it]                                                   {'loss': 0.5828, 'learning_rate': 9.549150281252633e-05, 'epoch': 0.81}
 81%|████████  | 474/588 [4:07:04<57:42, 30.38s/it] 81%|████████  | 475/588 [4:07:34<57:14, 30.40s/it]                                                   {'loss': 0.586, 'learning_rate': 9.387784632484825e-05, 'epoch': 0.81}
 81%|████████  | 475/588 [4:07:34<57:14, 30.40s/it] 81%|████████  | 476/588 [4:08:04<56:40, 30.36s/it]                                                   {'loss': 0.6229, 'learning_rate': 9.227652672497761e-05, 'epoch': 0.81}
 81%|████████  | 476/588 [4:08:04<56:40, 30.36s/it] 81%|████████  | 477/588 [4:08:35<56:06, 30.33s/it]                                                   {'loss': 0.6001, 'learning_rate': 9.068759265665382e-05, 'epoch': 0.81}
 81%|████████  | 477/588 [4:08:35<56:06, 30.33s/it] 81%|████████▏ | 478/588 [4:09:05<55:32, 30.30s/it]                                                   {'loss': 0.6443, 'learning_rate': 8.911109238737747e-05, 'epoch': 0.81}
 81%|████████▏ | 478/588 [4:09:05<55:32, 30.30s/it] 81%|████████▏ | 479/588 [4:09:35<55:06, 30.34s/it]                                                   {'loss': 0.5928, 'learning_rate': 8.754707380694426e-05, 'epoch': 0.81}
 81%|████████▏ | 479/588 [4:09:35<55:06, 30.34s/it] 82%|████████▏ | 480/588 [4:10:06<54:39, 30.36s/it]                                                   {'loss': 0.6671, 'learning_rate': 8.599558442598998e-05, 'epoch': 0.82}
 82%|████████▏ | 480/588 [4:10:06<54:39, 30.36s/it] 82%|████████▏ | 481/588 [4:10:36<54:13, 30.40s/it]                                                   {'loss': 0.6179, 'learning_rate': 8.44566713745476e-05, 'epoch': 0.82}
 82%|████████▏ | 481/588 [4:10:36<54:13, 30.40s/it] 82%|████████▏ | 482/588 [4:11:07<53:46, 30.44s/it]                                                   {'loss': 0.6188, 'learning_rate': 8.293038140061514e-05, 'epoch': 0.82}
 82%|████████▏ | 482/588 [4:11:07<53:46, 30.44s/it] 82%|████████▏ | 483/588 [4:11:37<53:15, 30.43s/it]                                                   {'loss': 0.6437, 'learning_rate': 8.141676086873573e-05, 'epoch': 0.82}
 82%|████████▏ | 483/588 [4:11:37<53:15, 30.43s/it] 82%|████████▏ | 484/588 [4:12:07<52:36, 30.35s/it]                                                   {'loss': 0.6008, 'learning_rate': 7.991585575858961e-05, 'epoch': 0.82}
 82%|████████▏ | 484/588 [4:12:07<52:36, 30.35s/it] 82%|████████▏ | 485/588 [4:12:40<53:17, 31.05s/it]                                                   {'loss': 0.6047, 'learning_rate': 7.84277116635968e-05, 'epoch': 0.82}
 82%|████████▏ | 485/588 [4:12:40<53:17, 31.05s/it] 83%|████████▎ | 486/588 [4:13:10<52:24, 30.83s/it]                                                   {'loss': 0.612, 'learning_rate': 7.695237378953224e-05, 'epoch': 0.83}
 83%|████████▎ | 486/588 [4:13:10<52:24, 30.83s/it] 83%|████████▎ | 487/588 [4:13:41<51:38, 30.68s/it]                                                   {'loss': 0.6568, 'learning_rate': 7.548988695315312e-05, 'epoch': 0.83}
 83%|████████▎ | 487/588 [4:13:41<51:38, 30.68s/it] 83%|████████▎ | 488/588 [4:14:14<52:18, 31.39s/it]                                                   {'loss': 0.6043, 'learning_rate': 7.404029558083653e-05, 'epoch': 0.83}
 83%|████████▎ | 488/588 [4:14:14<52:18, 31.39s/it] 83%|████████▎ | 489/588 [4:14:44<51:14, 31.06s/it]                                                   {'loss': 0.6019, 'learning_rate': 7.260364370723043e-05, 'epoch': 0.83}
 83%|████████▎ | 489/588 [4:14:44<51:14, 31.06s/it] 83%|████████▎ | 490/588 [4:15:14<50:27, 30.89s/it]                                                   {'loss': 0.5986, 'learning_rate': 7.117997497391648e-05, 'epoch': 0.83}
 83%|████████▎ | 490/588 [4:15:14<50:27, 30.89s/it] 84%|████████▎ | 491/588 [4:15:45<49:42, 30.75s/it]                                                   {'loss': 0.5901, 'learning_rate': 6.976933262808321e-05, 'epoch': 0.84}
 84%|████████▎ | 491/588 [4:15:45<49:42, 30.75s/it] 84%|████████▎ | 492/588 [4:16:16<49:35, 31.00s/it]                                                   {'loss': 0.5846, 'learning_rate': 6.837175952121305e-05, 'epoch': 0.84}
 84%|████████▎ | 492/588 [4:16:16<49:35, 31.00s/it] 84%|████████▍ | 493/588 [4:16:47<48:48, 30.82s/it]                                                   {'loss': 0.6127, 'learning_rate': 6.698729810778065e-05, 'epoch': 0.84}
 84%|████████▍ | 493/588 [4:16:47<48:48, 30.82s/it] 84%|████████▍ | 494/588 [4:17:17<48:01, 30.65s/it]                                                   {'loss': 0.6144, 'learning_rate': 6.561599044396288e-05, 'epoch': 0.84}
 84%|████████▍ | 494/588 [4:17:17<48:01, 30.65s/it] 84%|████████▍ | 495/588 [4:17:48<47:22, 30.57s/it]                                                   {'loss': 0.6068, 'learning_rate': 6.425787818636131e-05, 'epoch': 0.84}
 84%|████████▍ | 495/588 [4:17:48<47:22, 30.57s/it] 84%|████████▍ | 496/588 [4:18:18<46:47, 30.52s/it]                                                   {'loss': 0.6039, 'learning_rate': 6.291300259073723e-05, 'epoch': 0.84}
 84%|████████▍ | 496/588 [4:18:18<46:47, 30.52s/it] 85%|████████▍ | 497/588 [4:18:48<46:12, 30.47s/it]                                                   {'loss': 0.5984, 'learning_rate': 6.158140451075794e-05, 'epoch': 0.85}
 85%|████████▍ | 497/588 [4:18:48<46:12, 30.47s/it] 85%|████████▍ | 498/588 [4:19:18<45:14, 30.16s/it]                                                   {'loss': 0.6319, 'learning_rate': 6.026312439675552e-05, 'epoch': 0.85}
 85%|████████▍ | 498/588 [4:19:18<45:14, 30.16s/it] 85%|████████▍ | 499/588 [4:19:56<48:09, 32.46s/it]                                                   {'loss': 0.6066, 'learning_rate': 5.895820229449905e-05, 'epoch': 0.85}
 85%|████████▍ | 499/588 [4:19:56<48:09, 32.46s/it] 85%|████████▌ | 500/588 [4:20:26<46:43, 31.86s/it]                                                   {'loss': 0.598, 'learning_rate': 5.7666677843977054e-05, 'epoch': 0.85}
 85%|████████▌ | 500/588 [4:20:26<46:43, 31.86s/it] 85%|████████▌ | 501/588 [4:21:04<48:48, 33.66s/it]                                                   {'loss': 0.621, 'learning_rate': 5.63885902781941e-05, 'epoch': 0.85}
 85%|████████▌ | 501/588 [4:21:04<48:48, 33.66s/it] 85%|████████▌ | 502/588 [4:21:34<46:48, 32.66s/it]                                                   {'loss': 0.6005, 'learning_rate': 5.5123978421978474e-05, 'epoch': 0.85}
 85%|████████▌ | 502/588 [4:21:34<46:48, 32.66s/it] 86%|████████▌ | 503/588 [4:22:04<45:16, 31.96s/it]                                                   {'loss': 0.619, 'learning_rate': 5.387288069080298e-05, 'epoch': 0.86}
 86%|████████▌ | 503/588 [4:22:05<45:16, 31.96s/it] 86%|████████▌ | 504/588 [4:22:35<44:03, 31.48s/it]                                                   {'loss': 0.6501, 'learning_rate': 5.2635335089618264e-05, 'epoch': 0.86}
 86%|████████▌ | 504/588 [4:22:35<44:03, 31.48s/it] 86%|████████▌ | 505/588 [4:23:05<43:04, 31.14s/it]                                                   {'loss': 0.577, 'learning_rate': 5.141137921169792e-05, 'epoch': 0.86}
 86%|████████▌ | 505/588 [4:23:05<43:04, 31.14s/it] 86%|████████▌ | 506/588 [4:23:36<42:13, 30.89s/it]                                                   {'loss': 0.6172, 'learning_rate': 5.020105023749644e-05, 'epoch': 0.86}
 86%|████████▌ | 506/588 [4:23:36<42:13, 30.89s/it] 86%|████████▌ | 507/588 [4:24:06<41:30, 30.75s/it]                                                   {'loss': 0.6237, 'learning_rate': 4.900438493352055e-05, 'epoch': 0.86}
 86%|████████▌ | 507/588 [4:24:06<41:30, 30.75s/it] 86%|████████▋ | 508/588 [4:24:36<40:50, 30.63s/it]                                                   {'loss': 0.6197, 'learning_rate': 4.7821419651211284e-05, 'epoch': 0.86}
 86%|████████▋ | 508/588 [4:24:36<40:50, 30.63s/it] 87%|████████▋ | 509/588 [4:25:07<40:12, 30.53s/it]                                                   {'loss': 0.5749, 'learning_rate': 4.6652190325840395e-05, 'epoch': 0.87}
 87%|████████▋ | 509/588 [4:25:07<40:12, 30.53s/it] 87%|████████▋ | 510/588 [4:25:37<39:40, 30.52s/it]                                                   {'loss': 0.6307, 'learning_rate': 4.5496732475418745e-05, 'epoch': 0.87}
 87%|████████▋ | 510/588 [4:25:37<39:40, 30.52s/it] 87%|████████▋ | 511/588 [4:26:20<44:07, 34.38s/it]                                                   {'loss': 0.6453, 'learning_rate': 4.435508119961701e-05, 'epoch': 0.87}
 87%|████████▋ | 511/588 [4:26:20<44:07, 34.38s/it] 87%|████████▋ | 512/588 [4:26:51<42:08, 33.26s/it]                                                   {'loss': 0.6189, 'learning_rate': 4.322727117869951e-05, 'epoch': 0.87}
 87%|████████▋ | 512/588 [4:26:51<42:08, 33.26s/it] 87%|████████▋ | 513/588 [4:28:03<56:02, 44.83s/it]                                                   {'loss': 0.6062, 'learning_rate': 4.211333667247125e-05, 'epoch': 0.87}
 87%|████████▋ | 513/588 [4:28:03<56:02, 44.83s/it] 87%|████████▋ | 514/588 [4:28:33<49:56, 40.49s/it]                                                   {'loss': 0.6411, 'learning_rate': 4.101331151923648e-05, 'epoch': 0.87}
 87%|████████▋ | 514/588 [4:28:33<49:56, 40.49s/it] 88%|████████▊ | 515/588 [4:29:04<45:33, 37.44s/it]                                                   {'loss': 0.6063, 'learning_rate': 3.992722913477104e-05, 'epoch': 0.88}
 88%|████████▊ | 515/588 [4:29:04<45:33, 37.44s/it] 88%|████████▊ | 516/588 [4:29:34<42:25, 35.36s/it]                                                   {'loss': 0.6289, 'learning_rate': 3.885512251130763e-05, 'epoch': 0.88}
 88%|████████▊ | 516/588 [4:29:34<42:25, 35.36s/it] 88%|████████▊ | 517/588 [4:30:06<40:33, 34.27s/it]                                                   {'loss': 0.6371, 'learning_rate': 3.779702421653314e-05, 'epoch': 0.88}
 88%|████████▊ | 517/588 [4:30:06<40:33, 34.27s/it] 88%|████████▊ | 518/588 [4:30:36<38:33, 33.06s/it]                                                   {'loss': 0.6347, 'learning_rate': 3.675296639259912e-05, 'epoch': 0.88}
 88%|████████▊ | 518/588 [4:30:36<38:33, 33.06s/it] 88%|████████▊ | 519/588 [4:31:06<37:03, 32.22s/it]                                                   {'loss': 0.6255, 'learning_rate': 3.572298075514652e-05, 'epoch': 0.88}
 88%|████████▊ | 519/588 [4:31:06<37:03, 32.22s/it] 88%|████████▊ | 520/588 [4:31:37<35:54, 31.69s/it]                                                   {'loss': 0.6209, 'learning_rate': 3.470709859234083e-05, 'epoch': 0.88}
 88%|████████▊ | 520/588 [4:31:37<35:54, 31.69s/it] 89%|████████▊ | 521/588 [4:32:07<34:56, 31.30s/it]                                                   {'loss': 0.6268, 'learning_rate': 3.370535076392256e-05, 'epoch': 0.89}
 89%|████████▊ | 521/588 [4:32:07<34:56, 31.30s/it] 89%|████████▉ | 522/588 [4:32:38<34:06, 31.01s/it]                                                   {'loss': 0.5973, 'learning_rate': 3.271776770026963e-05, 'epoch': 0.89}
 89%|████████▉ | 522/588 [4:32:38<34:06, 31.01s/it] 89%|████████▉ | 523/588 [4:33:08<33:20, 30.77s/it]                                                   {'loss': 0.5862, 'learning_rate': 3.174437940147268e-05, 'epoch': 0.89}
 89%|████████▉ | 523/588 [4:33:08<33:20, 30.77s/it] 89%|████████▉ | 524/588 [4:33:38<32:40, 30.64s/it]                                                   {'loss': 0.6259, 'learning_rate': 3.078521543642399e-05, 'epoch': 0.89}
 89%|████████▉ | 524/588 [4:33:38<32:40, 30.64s/it] 89%|████████▉ | 525/588 [4:34:08<32:05, 30.56s/it]                                                   {'loss': 0.6345, 'learning_rate': 2.9840304941919415e-05, 'epoch': 0.89}
 89%|████████▉ | 525/588 [4:34:08<32:05, 30.56s/it] 89%|████████▉ | 526/588 [4:34:39<31:27, 30.45s/it]                                                   {'loss': 0.6145, 'learning_rate': 2.8909676621772853e-05, 'epoch': 0.89}
 89%|████████▉ | 526/588 [4:34:39<31:27, 30.45s/it] 90%|████████▉ | 527/588 [4:35:09<30:56, 30.43s/it]                                                   {'loss': 0.6137, 'learning_rate': 2.7993358745944607e-05, 'epoch': 0.9}
 90%|████████▉ | 527/588 [4:35:09<30:56, 30.43s/it] 90%|████████▉ | 528/588 [4:35:39<30:22, 30.38s/it]                                                   {'loss': 0.6078, 'learning_rate': 2.709137914968268e-05, 'epoch': 0.9}
 90%|████████▉ | 528/588 [4:35:39<30:22, 30.38s/it] 90%|████████▉ | 529/588 [4:36:10<29:51, 30.37s/it]                                                   {'loss': 0.6191, 'learning_rate': 2.6203765232676978e-05, 'epoch': 0.9}
 90%|████████▉ | 529/588 [4:36:10<29:51, 30.37s/it] 90%|█████████ | 530/588 [4:36:40<29:21, 30.37s/it]                                                   {'loss': 0.5804, 'learning_rate': 2.5330543958227037e-05, 'epoch': 0.9}
 90%|█████████ | 530/588 [4:36:40<29:21, 30.37s/it] 90%|█████████ | 531/588 [4:37:10<28:50, 30.36s/it]                                                   {'loss': 0.5792, 'learning_rate': 2.4471741852423235e-05, 'epoch': 0.9}
 90%|█████████ | 531/588 [4:37:10<28:50, 30.36s/it] 90%|█████████ | 532/588 [4:37:41<28:19, 30.34s/it]                                                   {'loss': 0.6066, 'learning_rate': 2.362738500334055e-05, 'epoch': 0.9}
 90%|█████████ | 532/588 [4:37:41<28:19, 30.34s/it] 91%|█████████ | 533/588 [4:38:11<27:47, 30.32s/it]                                                   {'loss': 0.6469, 'learning_rate': 2.2797499060246252e-05, 'epoch': 0.91}
 91%|█████████ | 533/588 [4:38:11<27:47, 30.32s/it] 91%|█████████ | 534/588 [4:38:41<27:17, 30.33s/it]                                                   {'loss': 0.6036, 'learning_rate': 2.198210923282118e-05, 'epoch': 0.91}
 91%|█████████ | 534/588 [4:38:41<27:17, 30.33s/it] 91%|█████████ | 535/588 [4:39:12<26:48, 30.35s/it]                                                   {'loss': 0.5913, 'learning_rate': 2.118124029039309e-05, 'epoch': 0.91}
 91%|█████████ | 535/588 [4:39:12<26:48, 30.35s/it] 91%|█████████ | 536/588 [4:39:42<26:15, 30.29s/it]                                                   {'loss': 0.6273, 'learning_rate': 2.0394916561185085e-05, 'epoch': 0.91}
 91%|█████████ | 536/588 [4:39:42<26:15, 30.29s/it] 91%|█████████▏| 537/588 [4:40:12<25:45, 30.30s/it]                                                   {'loss': 0.5685, 'learning_rate': 1.9623161931575928e-05, 'epoch': 0.91}
 91%|█████████▏| 537/588 [4:40:12<25:45, 30.30s/it] 91%|█████████▏| 538/588 [4:40:42<25:14, 30.29s/it]                                                   {'loss': 0.6053, 'learning_rate': 1.886599984537479e-05, 'epoch': 0.91}
 91%|█████████▏| 538/588 [4:40:42<25:14, 30.29s/it] 92%|█████████▏| 539/588 [4:41:13<24:43, 30.28s/it]                                                   {'loss': 0.5947, 'learning_rate': 1.812345330310916e-05, 'epoch': 0.92}
 92%|█████████▏| 539/588 [4:41:13<24:43, 30.28s/it] 92%|█████████▏| 540/588 [4:41:43<24:13, 30.29s/it]                                                   {'loss': 0.5973, 'learning_rate': 1.7395544861325717e-05, 'epoch': 0.92}
 92%|█████████▏| 540/588 [4:41:43<24:13, 30.29s/it] 92%|█████████▏| 541/588 [4:42:13<23:44, 30.30s/it]                                                   {'loss': 0.5679, 'learning_rate': 1.6682296631905624e-05, 'epoch': 0.92}
 92%|█████████▏| 541/588 [4:42:13<23:44, 30.30s/it] 92%|█████████▏| 542/588 [4:42:44<23:14, 30.32s/it]                                                   {'loss': 0.5689, 'learning_rate': 1.5983730281392662e-05, 'epoch': 0.92}
 92%|█████████▏| 542/588 [4:42:44<23:14, 30.32s/it] 92%|█████████▏| 543/588 [4:43:14<22:43, 30.31s/it]                                                   {'loss': 0.5953, 'learning_rate': 1.5299867030334813e-05, 'epoch': 0.92}
 92%|█████████▏| 543/588 [4:43:14<22:43, 30.31s/it] 93%|█████████▎| 544/588 [4:43:45<22:17, 30.39s/it]                                                   {'loss': 0.5691, 'learning_rate': 1.4630727652640008e-05, 'epoch': 0.93}
 93%|█████████▎| 544/588 [4:43:45<22:17, 30.39s/it] 93%|█████████▎| 545/588 [4:44:26<24:05, 33.62s/it]                                                   {'loss': 0.5965, 'learning_rate': 1.3976332474944841e-05, 'epoch': 0.93}
 93%|█████████▎| 545/588 [4:44:26<24:05, 33.62s/it] 93%|█████████▎| 546/588 [4:44:56<22:51, 32.65s/it]                                                   {'loss': 0.5786, 'learning_rate': 1.3336701375997129e-05, 'epoch': 0.93}
 93%|█████████▎| 546/588 [4:44:56<22:51, 32.65s/it] 93%|█████████▎| 547/588 [4:45:27<21:52, 32.00s/it]                                                   {'loss': 0.5698, 'learning_rate': 1.2711853786052107e-05, 'epoch': 0.93}
 93%|█████████▎| 547/588 [4:45:27<21:52, 32.00s/it] 93%|█████████▎| 548/588 [4:45:57<20:58, 31.45s/it]                                                   {'loss': 0.5915, 'learning_rate': 1.210180868628219e-05, 'epoch': 0.93}
 93%|█████████▎| 548/588 [4:45:57<20:58, 31.45s/it] 93%|█████████▎| 549/588 [4:46:27<20:11, 31.06s/it]                                                   {'loss': 0.6245, 'learning_rate': 1.1506584608200365e-05, 'epoch': 0.93}
 93%|█████████▎| 549/588 [4:46:27<20:11, 31.06s/it] 94%|█████████▎| 550/588 [4:46:57<19:31, 30.83s/it]                                                   {'loss': 0.578, 'learning_rate': 1.0926199633097156e-05, 'epoch': 0.94}
 94%|█████████▎| 550/588 [4:46:57<19:31, 30.83s/it] 94%|█████████▎| 551/588 [4:47:27<18:54, 30.66s/it]                                                   {'loss': 0.5787, 'learning_rate': 1.0360671391491606e-05, 'epoch': 0.94}
 94%|█████████▎| 551/588 [4:47:27<18:54, 30.66s/it] 94%|█████████▍| 552/588 [4:47:58<18:19, 30.55s/it]                                                   {'loss': 0.6111, 'learning_rate': 9.810017062595322e-06, 'epoch': 0.94}
 94%|█████████▍| 552/588 [4:47:58<18:19, 30.55s/it] 94%|█████████▍| 553/588 [4:48:28<17:47, 30.49s/it]                                                   {'loss': 0.6385, 'learning_rate': 9.274253373791064e-06, 'epoch': 0.94}
 94%|█████████▍| 553/588 [4:48:28<17:47, 30.49s/it] 94%|█████████▍| 554/588 [4:48:59<17:16, 30.48s/it]                                                   {'loss': 0.5877, 'learning_rate': 8.753396600124253e-06, 'epoch': 0.94}
 94%|█████████▍| 554/588 [4:48:59<17:16, 30.48s/it] 94%|█████████▍| 555/588 [4:49:29<16:44, 30.45s/it]                                                   {'loss': 0.5933, 'learning_rate': 8.247462563808816e-06, 'epoch': 0.94}
 94%|█████████▍| 555/588 [4:49:29<16:44, 30.45s/it] 95%|█████████▍| 556/588 [4:49:59<16:12, 30.40s/it]                                                   {'loss': 0.604, 'learning_rate': 7.756466633746407e-06, 'epoch': 0.95}
 95%|█████████▍| 556/588 [4:49:59<16:12, 30.40s/it] 95%|█████████▍| 557/588 [4:50:29<15:41, 30.36s/it]                                                   {'loss': 0.5861, 'learning_rate': 7.280423725059604e-06, 'epoch': 0.95}
 95%|█████████▍| 557/588 [4:50:29<15:41, 30.36s/it] 95%|█████████▍| 558/588 [4:51:00<15:08, 30.29s/it]                                                   {'loss': 0.6088, 'learning_rate': 6.819348298638839e-06, 'epoch': 0.95}
 95%|█████████▍| 558/588 [4:51:00<15:08, 30.29s/it] 95%|█████████▌| 559/588 [4:51:30<14:39, 30.33s/it]                                                   {'loss': 0.5853, 'learning_rate': 6.373254360703018e-06, 'epoch': 0.95}
 95%|█████████▌| 559/588 [4:51:30<14:39, 30.33s/it] 95%|█████████▌| 560/588 [4:52:00<14:08, 30.29s/it]                                                   {'loss': 0.6447, 'learning_rate': 5.942155462374199e-06, 'epoch': 0.95}
 95%|█████████▌| 560/588 [4:52:00<14:08, 30.29s/it] 95%|█████████▌| 561/588 [4:52:31<13:38, 30.33s/it]                                                   {'loss': 0.6206, 'learning_rate': 5.5260646992657535e-06, 'epoch': 0.95}
 95%|█████████▌| 561/588 [4:52:31<13:38, 30.33s/it] 96%|█████████▌| 562/588 [4:53:01<13:10, 30.39s/it]                                                   {'loss': 0.6194, 'learning_rate': 5.1249947110849625e-06, 'epoch': 0.96}
 96%|█████████▌| 562/588 [4:53:01<13:10, 30.39s/it] 96%|█████████▌| 563/588 [4:53:32<12:40, 30.41s/it]                                                   {'loss': 0.6183, 'learning_rate': 4.738957681248379e-06, 'epoch': 0.96}
 96%|█████████▌| 563/588 [4:53:32<12:40, 30.41s/it] 96%|█████████▌| 564/588 [4:54:01<12:05, 30.25s/it]                                                   {'loss': 0.6435, 'learning_rate': 4.367965336512402e-06, 'epoch': 0.96}
 96%|█████████▌| 564/588 [4:54:01<12:05, 30.25s/it] 96%|█████████▌| 565/588 [4:54:32<11:36, 30.30s/it]                                                   {'loss': 0.5932, 'learning_rate': 4.012028946616675e-06, 'epoch': 0.96}
 96%|█████████▌| 565/588 [4:54:32<11:36, 30.30s/it] 96%|█████████▋| 566/588 [4:55:02<11:06, 30.29s/it]                                                   {'loss': 0.5705, 'learning_rate': 3.6711593239417973e-06, 'epoch': 0.96}
 96%|█████████▋| 566/588 [4:55:02<11:06, 30.29s/it] 96%|█████████▋| 567/588 [4:55:33<10:37, 30.37s/it]                                                   {'loss': 0.6025, 'learning_rate': 3.3453668231809286e-06, 'epoch': 0.96}
 96%|█████████▋| 567/588 [4:55:33<10:37, 30.37s/it] 97%|█████████▋| 568/588 [4:56:03<10:07, 30.35s/it]                                                   {'loss': 0.6619, 'learning_rate': 3.034661341025258e-06, 'epoch': 0.97}
 97%|█████████▋| 568/588 [4:56:03<10:07, 30.35s/it] 97%|█████████▋| 569/588 [4:56:33<09:35, 30.29s/it]                                                   {'loss': 0.6431, 'learning_rate': 2.739052315863355e-06, 'epoch': 0.97}
 97%|█████████▋| 569/588 [4:56:33<09:35, 30.29s/it] 97%|█████████▋| 570/588 [4:57:04<09:07, 30.41s/it]                                                   {'loss': 0.6096, 'learning_rate': 2.458548727494292e-06, 'epoch': 0.97}
 97%|█████████▋| 570/588 [4:57:04<09:07, 30.41s/it] 97%|█████████▋| 571/588 [4:57:34<08:36, 30.36s/it]                                                   {'loss': 0.5696, 'learning_rate': 2.1931590968551906e-06, 'epoch': 0.97}
 97%|█████████▋| 571/588 [4:57:34<08:36, 30.36s/it] 97%|█████████▋| 572/588 [4:58:04<08:05, 30.34s/it]                                                   {'loss': 0.6075, 'learning_rate': 1.9428914857620438e-06, 'epoch': 0.97}
 97%|█████████▋| 572/588 [4:58:04<08:05, 30.34s/it] 97%|█████████▋| 573/588 [4:58:35<07:35, 30.35s/it]                                                   {'loss': 0.5934, 'learning_rate': 1.7077534966650766e-06, 'epoch': 0.97}
 97%|█████████▋| 573/588 [4:58:35<07:35, 30.35s/it] 98%|█████████▊| 574/588 [4:59:05<07:05, 30.37s/it]                                                   {'loss': 0.6031, 'learning_rate': 1.4877522724175975e-06, 'epoch': 0.98}
 98%|█████████▊| 574/588 [4:59:05<07:05, 30.37s/it] 98%|█████████▊| 575/588 [4:59:45<07:10, 33.13s/it]                                                   {'loss': 0.6107, 'learning_rate': 1.2828944960592836e-06, 'epoch': 0.98}
 98%|█████████▊| 575/588 [4:59:45<07:10, 33.13s/it] 98%|█████████▊| 576/588 [5:00:15<06:28, 32.34s/it]                                                   {'loss': 0.6041, 'learning_rate': 1.0931863906127326e-06, 'epoch': 0.98}
 98%|█████████▊| 576/588 [5:00:15<06:28, 32.34s/it] 98%|█████████▊| 577/588 [5:00:46<05:48, 31.72s/it]                                                   {'loss': 0.6415, 'learning_rate': 9.186337188949456e-07, 'epoch': 0.98}
 98%|█████████▊| 577/588 [5:00:46<05:48, 31.72s/it] 98%|█████████▊| 578/588 [5:01:16<05:13, 31.31s/it]                                                   {'loss': 0.6093, 'learning_rate': 7.592417833419129e-07, 'epoch': 0.98}
 98%|█████████▊| 578/588 [5:01:16<05:13, 31.31s/it] 98%|█████████▊| 579/588 [5:01:46<04:39, 31.09s/it]                                                   {'loss': 0.6106, 'learning_rate': 6.150154258476315e-07, 'epoch': 0.98}
 98%|█████████▊| 579/588 [5:01:46<04:39, 31.09s/it] 99%|█████████▊| 580/588 [5:02:17<04:06, 30.85s/it]                                                   {'loss': 0.6111, 'learning_rate': 4.859590276170556e-07, 'epoch': 0.99}
 99%|█████████▊| 580/588 [5:02:17<04:06, 30.85s/it] 99%|█████████▉| 581/588 [5:02:47<03:35, 30.72s/it]                                                   {'loss': 0.6093, 'learning_rate': 3.720765090329814e-07, 'epoch': 0.99}
 99%|█████████▉| 581/588 [5:02:47<03:35, 30.72s/it] 99%|█████████▉| 582/588 [5:03:18<03:03, 30.60s/it]                                                   {'loss': 0.5873, 'learning_rate': 2.733713295369755e-07, 'epoch': 0.99}
 99%|█████████▉| 582/588 [5:03:18<03:03, 30.60s/it] 99%|█████████▉| 583/588 [5:03:51<02:37, 31.54s/it]                                                   {'loss': 0.6113, 'learning_rate': 1.8984648752429223e-07, 'epoch': 0.99}
 99%|█████████▉| 583/588 [5:03:51<02:37, 31.54s/it] 99%|█████████▉| 584/588 [5:04:23<02:06, 31.71s/it]                                                   {'loss': 0.5745, 'learning_rate': 1.215045202527243e-07, 'epoch': 0.99}
 99%|█████████▉| 584/588 [5:04:23<02:06, 31.71s/it] 99%|█████████▉| 585/588 [5:04:58<01:37, 32.45s/it]                                                   {'loss': 0.5893, 'learning_rate': 6.834750376549792e-08, 'epoch': 0.99}
 99%|█████████▉| 585/588 [5:04:58<01:37, 32.45s/it]100%|█████████▉| 586/588 [5:05:28<01:03, 31.85s/it]                                                   {'loss': 0.6137, 'learning_rate': 3.037705282848968e-08, 'epoch': 1.0}
100%|█████████▉| 586/588 [5:05:28<01:03, 31.85s/it]100%|█████████▉| 587/588 [5:05:58<00:31, 31.36s/it]                                                   {'loss': 0.6478, 'learning_rate': 7.59432088082157e-09, 'epoch': 1.0}
100%|█████████▉| 587/588 [5:05:58<00:31, 31.36s/it]100%|██████████| 588/588 [5:06:29<00:00, 31.12s/it]                                                   {'loss': 0.5623, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 588/588 [5:06:29<00:00, 31.12s/it]                                                   {'train_runtime': 18389.2727, 'train_samples_per_second': 8.185, 'train_steps_per_second': 0.032, 'train_loss': 0.6702292711556364, 'epoch': 1.0}
100%|██████████| 588/588 [5:06:29<00:00, 31.12s/it]100%|██████████| 588/588 [5:06:29<00:00, 31.27s/it]
[2024-11-03 15:07:51,900] [INFO] [launch.py:347:main] Process 10419 exits successfully.
[2024-11-03 15:07:51,901] [INFO] [launch.py:347:main] Process 10422 exits successfully.
[2024-11-03 15:07:51,901] [INFO] [launch.py:347:main] Process 10417 exits successfully.
[2024-11-03 15:07:52,902] [INFO] [launch.py:347:main] Process 10420 exits successfully.
[2024-11-03 15:07:52,902] [INFO] [launch.py:347:main] Process 10415 exits successfully.
[2024-11-03 15:07:52,903] [INFO] [launch.py:347:main] Process 10418 exits successfully.
[2024-11-03 15:07:53,904] [INFO] [launch.py:347:main] Process 10421 exits successfully.
[2024-11-03 15:07:53,904] [INFO] [launch.py:347:main] Process 10416 exits successfully.
